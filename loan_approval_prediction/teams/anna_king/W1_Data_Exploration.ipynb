{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install pandas, seaborn, scikit-learn if you dont already have it\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "data_folder = os.getcwd().split(\"loan_approval_prediction\")[0] + \"loan_approval_prediction\\\\data\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_risk_df = pd.read_csv(data_folder + 'credit_risk_dataset.csv')  \n",
    "credit_risk_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = credit_risk_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: I need to do some data exploration in python. What are some things I need to check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start data exploration in Python, there are a few essential steps that will give you a strong understanding of your dataset. Here's a general checklist:\n",
    "\n",
    "1. Basic Information\n",
    "\n",
    "- Shape of the Data: Use .shape to check the number of rows and columns.\n",
    "- Data Types: Use .dtypes to inspect the types of each column.\n",
    "- Initial View: Display the first few rows using .head() and last few rows with .tail() to get an overview.\n",
    "\n",
    "2. Null and Missing Values\n",
    "- Check for Nulls: Use .isnull().sum() to see the count of missing values in each column.\n",
    "- Fill or Drop: Decide whether to fill or drop null values, depending on the analysis context.\n",
    "\n",
    "3. Statistical Summaries\n",
    "- Summary Statistics: Use .describe() for a quick look at mean, median, standard deviation, etc., for numerical columns.\n",
    "- Categorical Summaries: Use .value_counts() on categorical columns to see distributions.\n",
    "\n",
    "4. Data Distribution\n",
    "- Numerical Distribution: Use histograms (sns.histplot() or .plot.hist()) to visualize the spread of numerical data.\n",
    "- Box Plots: Use sns.boxplot() to identify outliers and the spread of numerical values.\n",
    "- Categorical Distributions: Use bar charts for categorical variables to see distribution (sns.countplot()).\n",
    "\n",
    "5. Correlation Analysis\n",
    "- Correlation Matrix: Use .corr() to see correlation coefficients between numerical variables, and visualize with a heatmap (sns.heatmap()).\n",
    "- Pair Plots: Use sns.pairplot() for visualizing pairwise relationships in the data.\n",
    "\n",
    "6. Identifying Duplicates\n",
    "- Duplicate Rows: Check for duplicate rows with .duplicated().sum().\n",
    "- Drop Duplicates: Use .drop_duplicates() if necessary.\n",
    "\n",
    "7. Data Types and Conversion\n",
    "- Data Type Inspection: Ensure data types are correct for each column (e.g., datetime columns should be in datetime format).\n",
    "- Convert Types: Use pd.to_datetime() for dates or .astype() for numerical or categorical conversions.\n",
    "\n",
    "8. Outliers Detection\n",
    "- Box Plots for Outliers: Revisit box plots to locate outliers in numerical data.\n",
    "- Z-Scores or IQR: Use Z-scores or the Interquartile Range (IQR) method to identify outliers programmatically.\n",
    "\n",
    "9. Feature Engineering Ideas\n",
    "- Create New Features: Look for opportunities to engineer new features from existing data (e.g., creating age from a birth date).\n",
    "- Label Encoding: Use LabelEncoder or pd.get_dummies() for categorical variables if planning for machine learning.\n",
    "\n",
    "10. Visualizations\n",
    "- Distribution Visualizations: For better understanding, use scatter plots, pair plots, or bar charts (sns.scatterplot(), sns.barplot()).\n",
    "- Time Series Analysis: If you have time-series data, use line plots (sns.lineplot()) for trends over time.\n",
    "\n",
    "\n",
    "These steps will give you a broad understanding of your data and reveal areas that may need cleaning, transforming, or further analysis. Let me know if you’d like specific code snippets for any of these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "# df = pd.read_csv(\"your_data.csv\")\n",
    "\n",
    "# 1. Basic Information\n",
    "print(\"Shape of the data:\", df.shape)\n",
    "print(\"\\nData Types:\\n\", df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFirst few rows:\\n\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLast few rows:\\n\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Null and Missing Values\n",
    "print(\"\\nMissing Values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Optional: Fill or drop nulls\n",
    "# df = df.dropna()  # Drop all rows with any null values\n",
    "# df['column_name'].fillna(df['column_name'].mean(), inplace=True)  # Fill nulls in a specific column with mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().reset_index().rename(columns={0:'Total'}).merge(\n",
    "    df[df['loan_status'] == 1].isnull().sum().reset_index().rename(columns={0:'Approved'})).merge(\n",
    "    df[df['loan_status'] == 0].isnull().sum().reset_index().rename(columns={0:'Rejected'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Statistical Summaries\n",
    "print(\"\\nSummary Statistics:\\n\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object').columns:\n",
    "    total = df[col].value_counts().reset_index().rename(columns={'count':'total'})\n",
    "    approved = df[df['loan_status'] == 1][col].value_counts().reset_index().rename(columns={'count':'approved'})\n",
    "    rejected = df[df['loan_status'] == 0][col].value_counts().reset_index().rename(columns={'count':'rejected'})\n",
    "    summary = total.merge(approved).merge(rejected)\n",
    "    summary['approved_percentage'] = round(summary['approved'] / summary['total'], 2)\n",
    "    print(f\"\\nValue Counts for {col}:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Data Distribution\n",
    "# Plot histograms for numerical columns\n",
    "df.select_dtypes(include=['float64', 'int64']).hist(bins=15, figsize=(15, 10))\n",
    "plt.suptitle(\"Histograms of Numerical Features\")\n",
    "plt.show()\n",
    "\n",
    "# Box Plot for each numerical feature\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df.select_dtypes(include=['float64', 'int64']))\n",
    "plt.title(\"Boxplot for Numerical Features\")\n",
    "plt.show()\n",
    "\n",
    "# Categorical Distribution\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.countplot(y=col, data=df, order=df[col].value_counts().index)\n",
    "    plt.title(f\"Count plot for {col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Identifying Duplicates\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {duplicate_rows}\")\n",
    "\n",
    "# Optional: Drop duplicates\n",
    "# df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all instances of duplicated rows\n",
    "all_duplicated_rows = df[df.duplicated(keep=False)]\n",
    "all_duplicated_rows.sort_values(['person_age', 'person_income', 'person_home_ownership',\n",
    "       'person_emp_length', 'loan_intent', 'loan_grade', 'loan_amnt',\n",
    "       'loan_int_rate', 'loan_status', 'loan_percent_income',\n",
    "       'cb_person_default_on_file', 'cb_person_cred_hist_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"duplicated\"] = df.duplicated(keep=False)\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# df.sort_values(['person_age', 'person_income', 'person_home_ownership',\n",
    "#        'person_emp_length', 'loan_intent', 'loan_grade', 'loan_amnt',\n",
    "#        'loan_int_rate', 'loan_status', 'loan_percent_income',\n",
    "#        'cb_person_default_on_file', 'cb_person_cred_hist_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 8. Outliers Detection\n",
    "# # Boxplot by feature to detect outliers\n",
    "# for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     sns.boxplot(x=col, data=df)\n",
    "#     plt.title(f\"Boxplot of {col}\")\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: This data will be used to model classification. Knowing this, is there anything additional that should be looked at during data exploration?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you’ll be using the data for classification, there are a few additional steps to improve your data exploration and ensure your dataset is optimized for building a robust model. Here’s what you should check:\n",
    "\n",
    "1. Class Balance\n",
    "- Check the balance of the target class to identify any class imbalance, as it can affect model performance.\n",
    "- Use value_counts() on your target column or plot it with a bar chart to visualize the class distribution.\n",
    "\n",
    "2. Feature Scaling\n",
    "- For algorithms sensitive to feature scaling (e.g., logistic regression, SVM, KNN), check the range of numerical features to ensure consistent scaling.\n",
    "- You can use .describe() to check the range and consider scaling (e.g., MinMaxScaler or StandardScaler) if ranges vary widely.\n",
    "\n",
    "3. Feature Correlation with Target\n",
    "- Measure the correlation of each feature with the target label to understand the potential predictive power of each feature.\n",
    "- For numerical features, you can use .corrwith(target_column), and for categorical, evaluate distributions within each class.\n",
    "\n",
    "4. Redundant and Low-Variance Features\n",
    "- Identify highly correlated features and consider dropping one to avoid multicollinearity, which can hinder some models.\n",
    "- Check for low-variance features, as they may not contribute to classification (use .var() or VarianceThreshold).\n",
    "\n",
    "5. Categorical Feature Encoding\n",
    "- Ensure all categorical variables are properly encoded for model training. Use pd.get_dummies() for one-hot encoding or LabelEncoder for ordinal data.\n",
    "- Avoid high cardinality categorical features, as they can introduce sparsity in the dataset.\n",
    "\n",
    "6. Handling Outliers Carefully\n",
    "- Outliers may affect models differently. For tree-based models, they may not be an issue, but for linear models, they can skew results.\n",
    "- Carefully decide on either treating or retaining outliers based on their impact on the model.\n",
    "\n",
    "7. Dimensionality Reduction (Optional)\n",
    "- Consider performing PCA or another dimensionality reduction method if you have many features, especially after one-hot encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Class Balance for 'loan_status'\n",
    "print(\"\\nClass Distribution for 'loan_status':\\n\", df['loan_status'].value_counts())\n",
    "sns.countplot(x='loan_status', data=df)\n",
    "plt.title(\"Class Balance for loan_status\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Correlation Analysis\n",
    "# Correlation with 'loan_status'\n",
    "numerical_features = df.select_dtypes(include=['float64', 'int64']).drop(columns=['loan_status'], errors='ignore')\n",
    "correlations = numerical_features.corrwith(df['loan_status'])\n",
    "print(\"\\nCorrelation with loan_status:\\n\", correlations)\n",
    "\n",
    "# Visualizing Correlation with Target\n",
    "correlations.plot(kind='bar', figsize=(10, 6), title=\"Feature Correlation with loan_status\")\n",
    "plt.show()\n",
    "\n",
    "# Redundant Features Check - Correlation Matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(numerical_features.corr(), annot=True, cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Low Variance Feature Detection\n",
    "# Apply VarianceThreshold\n",
    "selector = VarianceThreshold(threshold=0.1)\n",
    "selector.fit(df.select_dtypes(include=['float64', 'int64']))\n",
    "\n",
    "# Get the list of retained columns\n",
    "retained_columns = df.select_dtypes(include=['float64', 'int64']).columns[selector.get_support()]\n",
    "\n",
    "print(\"Retained Columns after Low Variance Filtering:\\n\", retained_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=['float64', 'int64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Encoding Categorical Features\n",
    "# Encode categorical features with LabelEncoder for simplicity\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = LabelEncoder().fit_transform(df[col])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Feature Scaling\n",
    "# Scaling numerical features for distance-based models\n",
    "# Select numerical columns excluding 'loan_status'\n",
    "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.drop('loan_status')\n",
    "\n",
    "# Scale only the selected numerical columns\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "# Assign the scaled features back to the original dataframe\n",
    "df[numerical_cols] = scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: Dimensionality Reduction with PCA\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=0.95)  # Retain 95% variance\n",
    "# df_pca = pca.fit_transform(df.drop(columns=['loan_status']))\n",
    "# print(f\"PCA reduced data shape: {df_pca.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loan_approval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
