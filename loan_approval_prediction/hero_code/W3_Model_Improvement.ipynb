{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS Workshop 3 - 29th November 2024\n",
    "## Team: Anna King\n",
    "\n",
    "In this notebook I attempt to improve on the model via:\n",
    "* Alternative models\n",
    "* Feature importance\n",
    "* Ensamble\n",
    "* Hyperparameter tuning\n",
    "* Outlier removal\n",
    "* Additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (0.13.2)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (from matplotlib) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.2-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading scikit_learn-1.6.1-cp311-cp311-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 6.6/11.1 MB 33.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 31.7 MB/s eta 0:00:00\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.15.2-cp311-cp311-win_amd64.whl (41.2 MB)\n",
      "   ---------------------------------------- 0.0/41.2 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 7.3/41.2 MB 34.9 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 14.9/41.2 MB 34.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 22.5/41.2 MB 35.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 30.4/41.2 MB 35.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.2/41.2 MB 39.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.2/41.2 MB 36.9 MB/s eta 0:00:00\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T16:12:09.334062Z",
     "iopub.status.busy": "2024-11-18T16:12:09.333416Z",
     "iopub.status.idle": "2024-11-18T16:12:10.926048Z",
     "shell.execute_reply": "2024-11-18T16:12:10.924856Z",
     "shell.execute_reply.started": "2024-11-18T16:12:09.334020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T16:12:10.927938Z",
     "iopub.status.busy": "2024-11-18T16:12:10.927403Z",
     "iopub.status.idle": "2024-11-18T16:12:11.042194Z",
     "shell.execute_reply": "2024-11-18T16:12:11.040985Z",
     "shell.execute_reply.started": "2024-11-18T16:12:10.927865Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_folder = os.getcwd().split(\"loan_approval_prediction\")[0] + \"loan_approval_prediction\\\\data\\\\\"\n",
    "df = pd.read_csv(data_folder + 'credit_risk_dataset.csv')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T16:12:13.258575Z",
     "iopub.status.busy": "2024-11-18T16:12:13.258177Z",
     "iopub.status.idle": "2024-11-18T16:12:13.310678Z",
     "shell.execute_reply": "2024-11-18T16:12:13.309588Z",
     "shell.execute_reply.started": "2024-11-18T16:12:13.258534Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. duplicates:  165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annak\\AppData\\Local\\Temp\\ipykernel_28596\\3771344972.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_final['cb_person_default_on_file'] = df_final['cb_person_default_on_file'].replace({'Y': 1, 'N': 0})\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(data_folder + 'credit_risk_dataset.csv')  \n",
    "\n",
    "# Identify duplicate rows\n",
    "duplicates = df.duplicated(keep='first')\n",
    "num_duplicates = duplicates.sum()\n",
    "\n",
    "# Remove duplicate rows\n",
    "df = df.drop_duplicates(keep='first')\n",
    "\n",
    "print('no. duplicates: ', num_duplicates)\n",
    "\n",
    "df['person_emp_length'] = df['person_emp_length'].fillna(-1)\n",
    "df['loan_int_rate'] = df['loan_int_rate'].fillna(-1)\n",
    "\n",
    "# We're also adding 'missing indicator' fields to explicity call out rows with missing data:\n",
    "df['missing_emp_length'] = (df['person_emp_length'] == -1).astype(int)\n",
    "df['missing_loan_rate'] = (df['loan_int_rate'] == -1).astype(int)\n",
    "\n",
    "df['person_age'] = df['person_age'].astype('uint8')\n",
    "df['person_income'] = df['person_income'].astype('uint32')\n",
    "df['loan_amnt'] = df['loan_amnt'].astype('uint32')\n",
    "df['loan_int_rate'] = df['loan_int_rate'].astype('float32')\n",
    "df['loan_status'] = df['loan_status'].astype('uint8')\n",
    "df['loan_percent_income'] = df['loan_percent_income'].astype('float32')\n",
    "df['cb_person_cred_hist_length'] = df['cb_person_cred_hist_length'].astype('uint8')\n",
    "df['missing_emp_length'] = df['missing_emp_length'].astype('uint8')\n",
    "df['missing_loan_rate'] = df['missing_loan_rate'].astype('uint8')\n",
    "\n",
    "# We are using min-max scaling for continuous variables as the underlying data is not normally distributed\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "df[['person_age', 'person_income','loan_amnt']] = min_max_scaler.fit_transform(df[['person_age','person_income','loan_amnt']])\n",
    "df.sample(5)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "\n",
    "encoded_data = encoder.fit_transform(df[['person_home_ownership','loan_intent','loan_grade']])\n",
    "\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['person_home_ownership','loan_intent','loan_grade']))\n",
    "df_final = pd.concat([df.drop(['person_home_ownership','loan_intent','loan_grade'], axis=1).reset_index(drop=True) , encoded_df.reset_index(drop=True) ], axis=1)\n",
    "\n",
    "df_final.sample(5)\n",
    "\n",
    "# Converting Ys and Ns to 1s and 0s\n",
    "df_final['cb_person_default_on_file'] = df_final['cb_person_default_on_file'].replace({'Y': 1, 'N': 0})\n",
    "df_final.head()\n",
    "\n",
    "# Mark NaN values in person_emp_length and loan_int_rate\n",
    "\n",
    "df_final['missing_emp_length'] = df_final['person_emp_length'].isnull().astype(int)\n",
    "df_final['missing_int_rate'] = df_final['loan_int_rate'].isnull().astype(int)\n",
    "\n",
    "# Also mark loans with 0 interest as a missing value\n",
    "df_final.loc[df_final['loan_int_rate'] == 0, 'missing_int_rate'] = 1\n",
    "\n",
    "# Let NaNs equal -1 to indicate missing value\n",
    "df_final.loc[df_final['person_emp_length'].isnull(), 'person_emp_length'] = -1\n",
    "df_final.loc[df_final['loan_int_rate'].isnull(), 'loan_int_rate'] = -1\n",
    "\n",
    "df_final[\"loan_rejected\"] =  1 - df_final['loan_status']\n",
    "df_final = df_final.drop(['loan_status'], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Step 1: Prepare the data (features and labels)\n",
    "X = df_final.drop('loan_rejected', axis=1)  # Features\n",
    "y = df_final['loan_rejected']  # Target label\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shap\n",
      "  Downloading shap-0.47.0-cp311-cp311-win_amd64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (from shap) (2.2.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (from shap) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (from shap) (1.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (from shap) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (from shap) (24.1)\n",
      "Collecting slicer==0.0.8 (from shap)\n",
      "  Using cached slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting numba>=0.54 (from shap)\n",
      "  Downloading numba-0.61.0-cp311-cp311-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting cloudpickle (from shap)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (from shap) (4.12.2)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.54->shap)\n",
      "  Downloading llvmlite-0.44.0-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting numpy (from shap)\n",
      "  Downloading numpy-2.1.3-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (from pandas->shap) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (from pandas->shap) (2025.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (from scikit-learn->shap) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (from scikit-learn->shap) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\annak\\onedrive\\documents\\github\\data_science_ai_workshop_series\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
      "Downloading shap-0.47.0-cp311-cp311-win_amd64.whl (530 kB)\n",
      "   ---------------------------------------- 0.0/530.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 530.3/530.3 kB 8.6 MB/s eta 0:00:00\n",
      "Using cached slicer-0.0.8-py3-none-any.whl (15 kB)\n",
      "Downloading numba-0.61.0-cp311-cp311-win_amd64.whl (2.8 MB)\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.8/2.8 MB 27.3 MB/s eta 0:00:00\n",
      "Downloading numpy-2.1.3-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 6.8/12.9 MB 35.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 32.3 MB/s eta 0:00:00\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading llvmlite-0.44.0-cp311-cp311-win_amd64.whl (30.3 MB)\n",
      "   ---------------------------------------- 0.0/30.3 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 7.3/30.3 MB 37.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 15.2/30.3 MB 38.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 23.6/30.3 MB 38.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  30.1/30.3 MB 38.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 30.3/30.3 MB 34.4 MB/s eta 0:00:00\n",
      "Installing collected packages: slicer, numpy, llvmlite, cloudpickle, numba, shap\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.3\n",
      "    Uninstalling numpy-2.2.3:\n",
      "      Successfully uninstalled numpy-2.2.3\n",
      "Successfully installed cloudpickle-3.1.1 llvmlite-0.44.0 numba-0.61.0 numpy-2.1.3 shap-0.47.0 slicer-0.0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\annak\\OneDrive\\Documents\\GitHub\\data_science_ai_workshop_series\\.conda\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\annak\\OneDrive\\Documents\\GitHub\\data_science_ai_workshop_series\\.conda\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        feature  Importance Mean  Importance Std\n",
      "5           loan_percent_income             0.09             0.0\n",
      "12   person_home_ownership_RENT             0.05             0.0\n",
      "1                 person_income             0.04             0.0\n",
      "20                 loan_grade_D             0.04             0.0\n",
      "4                 loan_int_rate             0.03             0.0\n",
      "14  loan_intent_HOMEIMPROVEMENT             0.02             0.0\n",
      "2             person_emp_length             0.01             0.0\n",
      "13        loan_intent_EDUCATION             0.01             0.0\n",
      "16         loan_intent_PERSONAL             0.01             0.0\n",
      "17          loan_intent_VENTURE             0.01             0.0\n",
      "21                 loan_grade_E             0.01             0.0\n",
      "19                 loan_grade_C             0.01             0.0\n",
      "15          loan_intent_MEDICAL             0.01             0.0\n",
      "11    person_home_ownership_OWN             0.00             0.0\n",
      "0                    person_age             0.00             0.0\n",
      "6     cb_person_default_on_file             0.00             0.0\n",
      "3                     loan_amnt             0.00             0.0\n",
      "22                 loan_grade_F             0.00             0.0\n",
      "23                 loan_grade_G             0.00             0.0\n",
      "8            missing_emp_length             0.00             0.0\n",
      "24             missing_int_rate             0.00             0.0\n",
      "10  person_home_ownership_OTHER            -0.00             0.0\n",
      "9             missing_loan_rate            -0.00             0.0\n",
      "7    cb_person_cred_hist_length            -0.00             0.0\n",
      "18                 loan_grade_B            -0.00             0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example: Train a Random Forest model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Compute permutation importance\n",
    "result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42, scoring='accuracy')\n",
    "\n",
    "# Accessing the results\n",
    "importances_mean = result.importances_mean\n",
    "importances_std = result.importances_std\n",
    "importances = result.importances\n",
    "\n",
    "# Create a DataFrame for better readability\n",
    "import pandas as pd\n",
    "feature_importances = pd.DataFrame({\n",
    "    'feature': X_test.columns,\n",
    "    'Importance Mean': importances_mean,\n",
    "    'Importance Std': importances_std\n",
    "}).sort_values(by='Importance Mean', ascending=False)\n",
    "\n",
    "print(round(feature_importances, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annak\\miniconda3\\envs\\loan_approval\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17758, number of negative: 4933\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000530 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 937\n",
      "[LightGBM] [Info] Number of data points in the train set: 22691, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.782601 -> initscore=1.280889\n",
      "[LightGBM] [Info] Start training from score 1.280889\n",
      "                 Model  Precision    Recall  f1_score  Accuracy  \\\n",
      "0  Logistic Regression   0.880799  0.955741  0.916741  0.864884   \n",
      "1                  SVM   0.839908  0.967631  0.899257  0.831260   \n",
      "2        Decision Tree   0.929997  0.923240  0.926606  0.886170   \n",
      "3        Random Forest   0.925985  0.990091  0.956966  0.930694   \n",
      "4              XGBoost   0.931365  0.987845  0.958774  0.933882   \n",
      "5             LightGBM   0.926250  0.993923  0.958894  0.933676   \n",
      "\n",
      "                                   Relevant_features  \n",
      "0  [loan_amnt, loan_int_rate, loan_percent_income...  \n",
      "1   [person_emp_length, loan_int_rate, loan_grade_D]  \n",
      "2  [person_age, person_income, person_emp_length,...  \n",
      "3  [person_income, person_emp_length, loan_int_ra...  \n",
      "4  [person_age, person_income, person_emp_length,...  \n",
      "5  [person_age, person_income, person_emp_length,...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# List of models to evaluate\n",
    "models = [\n",
    "    (\"Logistic Regression\", LogisticRegression()),\n",
    "    (\"SVM\", SVC()),\n",
    "    (\"Decision Tree\", DecisionTreeClassifier()),\n",
    "    (\"Random Forest\", RandomForestClassifier()),\n",
    "    (\"XGBoost\", XGBClassifier()),\n",
    "    (\"LightGBM\", LGBMClassifier())\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "feature_importance = pd.DataFrame()\n",
    "# Iterate through models\n",
    "for model_name, model in models:\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42, scoring='accuracy')\n",
    "\n",
    "    # Accessing the results\n",
    "    importances_mean = result.importances_mean\n",
    "    importances_std = result.importances_std\n",
    "    importances = result.importances\n",
    "\n",
    "    # Create a DataFrame for better readability\n",
    "    import pandas as pd\n",
    "    model_feature_importances = pd.DataFrame({\n",
    "        'feature': X_test.columns,\n",
    "        model_name + '_mean': importances_mean,\n",
    "        model_name + '_std': importances_std\n",
    "        \n",
    "    })\n",
    "\n",
    "    relevent_features = model_feature_importances[(model_feature_importances[model_name + '_mean'] > 0.005)][\"feature\"].tolist()\n",
    "\n",
    "    # Append results to the list\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Relevant_features\": relevent_features\n",
    "    })\n",
    "\n",
    "    if feature_importance.shape[0] > 0:\n",
    "        feature_importance = feature_importance.merge(model_feature_importances, \"outer\", on=\"feature\")\n",
    "    else:\n",
    "        feature_importance = model_feature_importances.copy()\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Jinja2 in c:\\users\\annak\\miniconda3\\envs\\loan_approval\\lib\\site-packages (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\annak\\miniconda3\\envs\\loan_approval\\lib\\site-packages (from Jinja2) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install Jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_67e99_row0_col0, #T_67e99_row0_col3, #T_67e99_row1_col2, #T_67e99_row1_col3, #T_67e99_row2_col5, #T_67e99_row3_col0, #T_67e99_row3_col3, #T_67e99_row4_col0, #T_67e99_row7_col4, #T_67e99_row7_col5, #T_67e99_row8_col2, #T_67e99_row11_col0, #T_67e99_row12_col0, #T_67e99_row16_col0, #T_67e99_row16_col3, #T_67e99_row17_col0, #T_67e99_row17_col3, #T_67e99_row18_col3, #T_67e99_row19_col0, #T_67e99_row21_col0, #T_67e99_row21_col3, #T_67e99_row24_col0 {\n",
       "  color: #f2f2dc;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_67e99_row0_col1, #T_67e99_row1_col1, #T_67e99_row4_col1, #T_67e99_row6_col1, #T_67e99_row11_col1, #T_67e99_row12_col1, #T_67e99_row12_col4, #T_67e99_row14_col1, #T_67e99_row19_col3 {\n",
       "  color: #f2f2d6;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_67e99_row0_col2 {\n",
       "  color: #f2f2d5;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_67e99_row0_col4, #T_67e99_row0_col5, #T_67e99_row1_col0, #T_67e99_row1_col4, #T_67e99_row1_col5, #T_67e99_row2_col1, #T_67e99_row2_col3, #T_67e99_row3_col1, #T_67e99_row3_col4, #T_67e99_row3_col5, #T_67e99_row7_col1, #T_67e99_row8_col1, #T_67e99_row16_col1, #T_67e99_row16_col2, #T_67e99_row16_col4, #T_67e99_row16_col5, #T_67e99_row17_col1, #T_67e99_row17_col2, #T_67e99_row17_col4, #T_67e99_row17_col5, #T_67e99_row18_col1, #T_67e99_row18_col2, #T_67e99_row18_col4, #T_67e99_row18_col5, #T_67e99_row19_col1, #T_67e99_row21_col1, #T_67e99_row21_col2, #T_67e99_row21_col4, #T_67e99_row21_col5, #T_67e99_row24_col1 {\n",
       "  color: #f2f2dd;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_67e99_row2_col0 {\n",
       "  color: #f4f4c0;\n",
       "}\n",
       "#T_67e99_row2_col2, #T_67e99_row20_col2 {\n",
       "  color: #f5f5ad;\n",
       "}\n",
       "#T_67e99_row2_col4, #T_67e99_row4_col3, #T_67e99_row6_col3, #T_67e99_row6_col4, #T_67e99_row22_col5 {\n",
       "  color: #f3f3cd;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_67e99_row3_col2, #T_67e99_row7_col2 {\n",
       "  color: #f2f2da;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_67e99_row4_col2 {\n",
       "  color: #f5f5ab;\n",
       "}\n",
       "#T_67e99_row4_col4, #T_67e99_row12_col5, #T_67e99_row22_col3 {\n",
       "  color: #f3f3cf;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_67e99_row4_col5 {\n",
       "  color: #f4f4ba;\n",
       "}\n",
       "#T_67e99_row5_col0 {\n",
       "  color: #f6f692;\n",
       "}\n",
       "#T_67e99_row5_col1 {\n",
       "  color: #f9f95f;\n",
       "}\n",
       "#T_67e99_row5_col2, #T_67e99_row20_col1 {\n",
       "  color: #f7f78d;\n",
       "}\n",
       "#T_67e99_row5_col3 {\n",
       "  color: #f8f879;\n",
       "}\n",
       "#T_67e99_row5_col4, #T_67e99_row9_col4, #T_67e99_row24_col3 {\n",
       "  color: #f7f786;\n",
       "}\n",
       "#T_67e99_row5_col5 {\n",
       "  color: #f6f696;\n",
       "}\n",
       "#T_67e99_row6_col0 {\n",
       "  color: #f3f3c4;\n",
       "}\n",
       "#T_67e99_row6_col2 {\n",
       "  color: #f3f3c8;\n",
       "}\n",
       "#T_67e99_row6_col5, #T_67e99_row10_col0 {\n",
       "  color: #f3f3d2;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_67e99_row7_col0, #T_67e99_row13_col0 {\n",
       "  color: #f3f3d5;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_67e99_row7_col3, #T_67e99_row8_col3, #T_67e99_row8_col4, #T_67e99_row8_col5, #T_67e99_row20_col0 {\n",
       "  color: #f2f2d9;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_67e99_row8_col0 {\n",
       "  color: #f2f2d7;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_67e99_row9_col0 {\n",
       "  color: #f6f6a0;\n",
       "}\n",
       "#T_67e99_row9_col1, #T_67e99_row15_col0, #T_67e99_row15_col2, #T_67e99_row15_col3, #T_67e99_row15_col4, #T_67e99_row15_col5 {\n",
       "  color: #ffff00;\n",
       "}\n",
       "#T_67e99_row9_col2 {\n",
       "  color: #fafa59;\n",
       "}\n",
       "#T_67e99_row9_col3 {\n",
       "  color: #f6f69a;\n",
       "}\n",
       "#T_67e99_row9_col5 {\n",
       "  color: #f9f96d;\n",
       "}\n",
       "#T_67e99_row10_col1, #T_67e99_row15_col1, #T_67e99_row22_col4 {\n",
       "  color: #f3f3c8;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_67e99_row10_col2, #T_67e99_row13_col1, #T_67e99_row23_col1 {\n",
       "  color: #f3f3ce;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_67e99_row10_col3, #T_67e99_row10_col5, #T_67e99_row14_col5, #T_67e99_row19_col5 {\n",
       "  color: #f4f4c3;\n",
       "}\n",
       "#T_67e99_row10_col4, #T_67e99_row14_col4 {\n",
       "  color: #f4f4c1;\n",
       "}\n",
       "#T_67e99_row11_col2 {\n",
       "  color: #f5f5a7;\n",
       "}\n",
       "#T_67e99_row11_col3, #T_67e99_row11_col4 {\n",
       "  color: #f5f5a6;\n",
       "}\n",
       "#T_67e99_row11_col5 {\n",
       "  color: #f5f5a2;\n",
       "}\n",
       "#T_67e99_row12_col2, #T_67e99_row22_col2 {\n",
       "  color: #f3f3d4;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_67e99_row12_col3 {\n",
       "  color: #f3f3ca;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_67e99_row13_col2, #T_67e99_row22_col0 {\n",
       "  color: #f3f3cb;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_67e99_row13_col3, #T_67e99_row14_col3 {\n",
       "  color: #f3f3c6;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_67e99_row13_col4 {\n",
       "  color: #f3f3c5;\n",
       "}\n",
       "#T_67e99_row13_col5 {\n",
       "  color: #f3f3c6;\n",
       "}\n",
       "#T_67e99_row14_col0 {\n",
       "  color: #f3f3d0;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_67e99_row14_col2 {\n",
       "  color: #f3f3c9;\n",
       "}\n",
       "#T_67e99_row18_col0, #T_67e99_row19_col4 {\n",
       "  color: #f4f4be;\n",
       "}\n",
       "#T_67e99_row19_col2 {\n",
       "  color: #f4f4b9;\n",
       "}\n",
       "#T_67e99_row20_col3, #T_67e99_row20_col5, #T_67e99_row23_col0 {\n",
       "  color: #f4f4b7;\n",
       "}\n",
       "#T_67e99_row20_col4 {\n",
       "  color: #f4f4bb;\n",
       "}\n",
       "#T_67e99_row22_col1 {\n",
       "  color: #f4f4c0;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_67e99_row23_col2 {\n",
       "  color: #f8f873;\n",
       "}\n",
       "#T_67e99_row23_col3 {\n",
       "  color: #fafa5b;\n",
       "}\n",
       "#T_67e99_row23_col4 {\n",
       "  color: #f9f968;\n",
       "}\n",
       "#T_67e99_row23_col5, #T_67e99_row24_col4 {\n",
       "  color: #f9f964;\n",
       "}\n",
       "#T_67e99_row24_col2 {\n",
       "  color: #f9f95e;\n",
       "}\n",
       "#T_67e99_row24_col5 {\n",
       "  color: #f7f783;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_67e99\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_67e99_level0_col0\" class=\"col_heading level0 col0\" >Logistic Regression_mean</th>\n",
       "      <th id=\"T_67e99_level0_col1\" class=\"col_heading level0 col1\" >SVM_mean</th>\n",
       "      <th id=\"T_67e99_level0_col2\" class=\"col_heading level0 col2\" >Decision Tree_mean</th>\n",
       "      <th id=\"T_67e99_level0_col3\" class=\"col_heading level0 col3\" >Random Forest_mean</th>\n",
       "      <th id=\"T_67e99_level0_col4\" class=\"col_heading level0 col4\" >XGBoost_mean</th>\n",
       "      <th id=\"T_67e99_level0_col5\" class=\"col_heading level0 col5\" >LightGBM_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >feature</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_67e99_level0_row0\" class=\"row_heading level0 row0\" >cb_person_cred_hist_length</th>\n",
       "      <td id=\"T_67e99_row0_col0\" class=\"data row0 col0\" >0.00</td>\n",
       "      <td id=\"T_67e99_row0_col1\" class=\"data row0 col1\" >0.00</td>\n",
       "      <td id=\"T_67e99_row0_col2\" class=\"data row0 col2\" >0.00</td>\n",
       "      <td id=\"T_67e99_row0_col3\" class=\"data row0 col3\" >-0.00</td>\n",
       "      <td id=\"T_67e99_row0_col4\" class=\"data row0 col4\" >0.00</td>\n",
       "      <td id=\"T_67e99_row0_col5\" class=\"data row0 col5\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67e99_level0_row1\" class=\"row_heading level0 row1\" >cb_person_default_on_file</th>\n",
       "      <td id=\"T_67e99_row1_col0\" class=\"data row1 col0\" >-0.00</td>\n",
       "      <td id=\"T_67e99_row1_col1\" class=\"data row1 col1\" >0.00</td>\n",
       "      <td id=\"T_67e99_row1_col2\" class=\"data row1 col2\" >0.00</td>\n",
       "      <td id=\"T_67e99_row1_col3\" class=\"data row1 col3\" >0.00</td>\n",
       "      <td id=\"T_67e99_row1_col4\" class=\"data row1 col4\" >-0.00</td>\n",
       "      <td id=\"T_67e99_row1_col5\" class=\"data row1 col5\" >-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67e99_level0_row2\" class=\"row_heading level0 row2\" >loan_amnt</th>\n",
       "      <td id=\"T_67e99_row2_col0\" class=\"data row2 col0\" >0.01</td>\n",
       "      <td id=\"T_67e99_row2_col1\" class=\"data row2 col1\" >0.00</td>\n",
       "      <td id=\"T_67e99_row2_col2\" class=\"data row2 col2\" >0.02</td>\n",
       "      <td id=\"T_67e99_row2_col3\" class=\"data row2 col3\" >-0.00</td>\n",
       "      <td id=\"T_67e99_row2_col4\" class=\"data row2 col4\" >0.01</td>\n",
       "      <td id=\"T_67e99_row2_col5\" class=\"data row2 col5\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67e99_level0_row3\" class=\"row_heading level0 row3\" >loan_grade_B</th>\n",
       "      <td id=\"T_67e99_row3_col0\" class=\"data row3 col0\" >-0.00</td>\n",
       "      <td id=\"T_67e99_row3_col1\" class=\"data row3 col1\" >-0.00</td>\n",
       "      <td id=\"T_67e99_row3_col2\" class=\"data row3 col2\" >0.00</td>\n",
       "      <td id=\"T_67e99_row3_col3\" class=\"data row3 col3\" >-0.00</td>\n",
       "      <td id=\"T_67e99_row3_col4\" class=\"data row3 col4\" >0.00</td>\n",
       "      <td id=\"T_67e99_row3_col5\" class=\"data row3 col5\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67e99_level0_row4\" class=\"row_heading level0 row4\" >loan_grade_C</th>\n",
       "      <td id=\"T_67e99_row4_col0\" class=\"data row4 col0\" >-0.00</td>\n",
       "      <td id=\"T_67e99_row4_col1\" class=\"data row4 col1\" >0.00</td>\n",
       "      <td id=\"T_67e99_row4_col2\" class=\"data row4 col2\" >0.03</td>\n",
       "      <td id=\"T_67e99_row4_col3\" class=\"data row4 col3\" >0.01</td>\n",
       "      <td id=\"T_67e99_row4_col4\" class=\"data row4 col4\" >0.01</td>\n",
       "      <td id=\"T_67e99_row4_col5\" class=\"data row4 col5\" >0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67e99_level0_row5\" class=\"row_heading level0 row5\" >loan_grade_D</th>\n",
       "      <td id=\"T_67e99_row5_col0\" class=\"data row5 col0\" >0.03</td>\n",
       "      <td id=\"T_67e99_row5_col1\" class=\"data row5 col1\" >0.02</td>\n",
       "      <td id=\"T_67e99_row5_col2\" class=\"data row5 col2\" >0.04</td>\n",
       "      <td id=\"T_67e99_row5_col3\" class=\"data row5 col3\" >0.04</td>\n",
       "      <td id=\"T_67e99_row5_col4\" class=\"data row5 col4\" >0.04</td>\n",
       "      <td id=\"T_67e99_row5_col5\" class=\"data row5 col5\" >0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67e99_level0_row6\" class=\"row_heading level0 row6\" >loan_grade_E</th>\n",
       "      <td id=\"T_67e99_row6_col0\" class=\"data row6 col0\" >0.01</td>\n",
       "      <td id=\"T_67e99_row6_col1\" class=\"data row6 col1\" >0.00</td>\n",
       "      <td id=\"T_67e99_row6_col2\" class=\"data row6 col2\" >0.01</td>\n",
       "      <td id=\"T_67e99_row6_col3\" class=\"data row6 col3\" >0.01</td>\n",
       "      <td id=\"T_67e99_row6_col4\" class=\"data row6 col4\" >0.01</td>\n",
       "      <td id=\"T_67e99_row6_col5\" class=\"data row6 col5\" >0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67e99_level0_row7\" class=\"row_heading level0 row7\" >loan_grade_F</th>\n",
       "      <td id=\"T_67e99_row7_col0\" class=\"data row7 col0\" >0.00</td>\n",
       "      <td id=\"T_67e99_row7_col1\" class=\"data row7 col1\" >-0.00</td>\n",
       "      <td id=\"T_67e99_row7_col2\" class=\"data row7 col2\" >0.00</td>\n",
       "      <td id=\"T_67e99_row7_col3\" class=\"data row7 col3\" >0.00</td>\n",
       "      <td id=\"T_67e99_row7_col4\" class=\"data row7 col4\" >0.00</td>\n",
       "      <td id=\"T_67e99_row7_col5\" class=\"data row7 col5\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67e99_level0_row8\" class=\"row_heading level0 row8\" >loan_grade_G</th>\n",
       "      <td id=\"T_67e99_row8_col0\" class=\"data row8 col0\" >0.00</td>\n",
       "      <td id=\"T_67e99_row8_col1\" class=\"data row8 col1\" >0.00</td>\n",
       "      <td id=\"T_67e99_row8_col2\" class=\"data row8 col2\" >0.00</td>\n",
       "      <td id=\"T_67e99_row8_col3\" class=\"data row8 col3\" >0.00</td>\n",
       "      <td id=\"T_67e99_row8_col4\" class=\"data row8 col4\" >0.00</td>\n",
       "      <td id=\"T_67e99_row8_col5\" class=\"data row8 col5\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67e99_level0_row9\" class=\"row_heading level0 row9\" >loan_int_rate</th>\n",
       "      <td id=\"T_67e99_row9_col0\" class=\"data row9 col0\" >0.03</td>\n",
       "      <td id=\"T_67e99_row9_col1\" class=\"data row9 col1\" >0.03</td>\n",
       "      <td id=\"T_67e99_row9_col2\" class=\"data row9 col2\" >0.07</td>\n",
       "      <td id=\"T_67e99_row9_col3\" class=\"data row9 col3\" >0.03</td>\n",
       "      <td id=\"T_67e99_row9_col4\" class=\"data row9 col4\" >0.04</td>\n",
       "      <td id=\"T_67e99_row9_col5\" class=\"data row9 col5\" >0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67e99_level0_row10\" class=\"row_heading level0 row10\" >loan_intent_EDUCATION</th>\n",
       "      <td id=\"T_67e99_row10_col0\" class=\"data row10 col0\" >0.00</td>\n",
       "      <td id=\"T_67e99_row10_col1\" class=\"data row10 col1\" >0.00</td>\n",
       "      <td id=\"T_67e99_row10_col2\" class=\"data row10 col2\" >0.01</td>\n",
       "      <td id=\"T_67e99_row10_col3\" class=\"data row10 col3\" >0.01</td>\n",
       "      <td id=\"T_67e99_row10_col4\" class=\"data row10 col4\" >0.01</td>\n",
       "      <td id=\"T_67e99_row10_col5\" class=\"data row10 col5\" >0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67e99_level0_row11\" class=\"row_heading level0 row11\" >loan_intent_HOMEIMPROVEMENT</th>\n",
       "      <td id=\"T_67e99_row11_col0\" class=\"data row11 col0\" >0.00</td>\n",
       "      <td id=\"T_67e99_row11_col1\" class=\"data row11 col1\" >0.00</td>\n",
       "      <td id=\"T_67e99_row11_col2\" class=\"data row11 col2\" >0.03</td>\n",
       "      <td id=\"T_67e99_row11_col3\" class=\"data row11 col3\" >0.02</td>\n",
       "      <td id=\"T_67e99_row11_col4\" class=\"data row11 col4\" >0.02</td>\n",
       "      <td id=\"T_67e99_row11_col5\" class=\"data row11 col5\" >0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67e99_level0_row12\" class=\"row_heading level0 row12\" >loan_intent_MEDICAL</th>\n",
       "      <td id=\"T_67e99_row12_col0\" class=\"data row12 col0\" >0.00</td>\n",
       "      <td id=\"T_67e99_row12_col1\" class=\"data row12 col1\" >0.00</td>\n",
       "      <td id=\"T_67e99_row12_col2\" class=\"data row12 col2\" >0.01</td>\n",
       "      <td id=\"T_67e99_row12_col3\" class=\"data row12 col3\" >0.01</td>\n",
       "      <td id=\"T_67e99_row12_col4\" class=\"data row12 col4\" >0.00</td>\n",
       "      <td id=\"T_67e99_row12_col5\" class=\"data row12 col5\" >0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67e99_level0_row13\" class=\"row_heading level0 row13\" >loan_intent_PERSONAL</th>\n",
       "      <td id=\"T_67e99_row13_col0\" class=\"data row13 col0\" >0.00</td>\n",
       "      <td id=\"T_67e99_row13_col1\" class=\"data row13 col1\" >0.00</td>\n",
       "      <td id=\"T_67e99_row13_col2\" class=\"data row13 col2\" >0.01</td>\n",
       "      <td id=\"T_67e99_row13_col3\" class=\"data row13 col3\" >0.01</td>\n",
       "      <td id=\"T_67e99_row13_col4\" class=\"data row13 col4\" >0.01</td>\n",
       "      <td id=\"T_67e99_row13_col5\" class=\"data row13 col5\" >0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67e99_level0_row14\" class=\"row_heading level0 row14\" >loan_intent_VENTURE</th>\n",
       "      <td id=\"T_67e99_row14_col0\" class=\"data row14 col0\" >0.01</td>\n",
       "      <td id=\"T_67e99_row14_col1\" class=\"data row14 col1\" >0.00</td>\n",
       "      <td id=\"T_67e99_row14_col2\" class=\"data row14 col2\" >0.01</td>\n",
       "      <td id=\"T_67e99_row14_col3\" class=\"data row14 col3\" >0.01</td>\n",
       "      <td id=\"T_67e99_row14_col4\" class=\"data row14 col4\" >0.01</td>\n",
       "      <td id=\"T_67e99_row14_col5\" class=\"data row14 col5\" >0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67e99_level0_row15\" class=\"row_heading level0 row15\" >loan_percent_income</th>\n",
       "      <td id=\"T_67e99_row15_col0\" class=\"data row15 col0\" >0.10</td>\n",
       "      <td id=\"T_67e99_row15_col1\" class=\"data row15 col1\" >0.00</td>\n",
       "      <td id=\"T_67e99_row15_col2\" class=\"data row15 col2\" >0.11</td>\n",
       "      <td id=\"T_67e99_row15_col3\" class=\"data row15 col3\" >0.09</td>\n",
       "      <td id=\"T_67e99_row15_col4\" class=\"data row15 col4\" >0.09</td>\n",
       "      <td id=\"T_67e99_row15_col5\" class=\"data row15 col5\" >0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67e99_level0_row16\" class=\"row_heading level0 row16\" >missing_emp_length</th>\n",
       "      <td id=\"T_67e99_row16_col0\" class=\"data row16 col0\" >0.00</td>\n",
       "      <td id=\"T_67e99_row16_col1\" class=\"data row16 col1\" >0.00</td>\n",
       "      <td id=\"T_67e99_row16_col2\" class=\"data row16 col2\" >0.00</td>\n",
       "      <td id=\"T_67e99_row16_col3\" class=\"data row16 col3\" >0.00</td>\n",
       "      <td id=\"T_67e99_row16_col4\" class=\"data row16 col4\" >0.00</td>\n",
       "      <td id=\"T_67e99_row16_col5\" class=\"data row16 col5\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67e99_level0_row17\" class=\"row_heading level0 row17\" >missing_int_rate</th>\n",
       "      <td id=\"T_67e99_row17_col0\" class=\"data row17 col0\" >0.00</td>\n",
       "      <td id=\"T_67e99_row17_col1\" class=\"data row17 col1\" >0.00</td>\n",
       "      <td id=\"T_67e99_row17_col2\" class=\"data row17 col2\" >0.00</td>\n",
       "      <td id=\"T_67e99_row17_col3\" class=\"data row17 col3\" >0.00</td>\n",
       "      <td id=\"T_67e99_row17_col4\" class=\"data row17 col4\" >0.00</td>\n",
       "      <td id=\"T_67e99_row17_col5\" class=\"data row17 col5\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67e99_level0_row18\" class=\"row_heading level0 row18\" >missing_loan_rate</th>\n",
       "      <td id=\"T_67e99_row18_col0\" class=\"data row18 col0\" >0.01</td>\n",
       "      <td id=\"T_67e99_row18_col1\" class=\"data row18 col1\" >0.00</td>\n",
       "      <td id=\"T_67e99_row18_col2\" class=\"data row18 col2\" >-0.00</td>\n",
       "      <td id=\"T_67e99_row18_col3\" class=\"data row18 col3\" >-0.00</td>\n",
       "      <td id=\"T_67e99_row18_col4\" class=\"data row18 col4\" >0.00</td>\n",
       "      <td id=\"T_67e99_row18_col5\" class=\"data row18 col5\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67e99_level0_row19\" class=\"row_heading level0 row19\" >person_age</th>\n",
       "      <td id=\"T_67e99_row19_col0\" class=\"data row19 col0\" >0.00</td>\n",
       "      <td id=\"T_67e99_row19_col1\" class=\"data row19 col1\" >-0.00</td>\n",
       "      <td id=\"T_67e99_row19_col2\" class=\"data row19 col2\" >0.02</td>\n",
       "      <td id=\"T_67e99_row19_col3\" class=\"data row19 col3\" >0.00</td>\n",
       "      <td id=\"T_67e99_row19_col4\" class=\"data row19 col4\" >0.01</td>\n",
       "      <td id=\"T_67e99_row19_col5\" class=\"data row19 col5\" >0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67e99_level0_row20\" class=\"row_heading level0 row20\" >person_emp_length</th>\n",
       "      <td id=\"T_67e99_row20_col0\" class=\"data row20 col0\" >0.00</td>\n",
       "      <td id=\"T_67e99_row20_col1\" class=\"data row20 col1\" >0.01</td>\n",
       "      <td id=\"T_67e99_row20_col2\" class=\"data row20 col2\" >0.02</td>\n",
       "      <td id=\"T_67e99_row20_col3\" class=\"data row20 col3\" >0.01</td>\n",
       "      <td id=\"T_67e99_row20_col4\" class=\"data row20 col4\" >0.01</td>\n",
       "      <td id=\"T_67e99_row20_col5\" class=\"data row20 col5\" >0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67e99_level0_row21\" class=\"row_heading level0 row21\" >person_home_ownership_OTHER</th>\n",
       "      <td id=\"T_67e99_row21_col0\" class=\"data row21 col0\" >-0.00</td>\n",
       "      <td id=\"T_67e99_row21_col1\" class=\"data row21 col1\" >0.00</td>\n",
       "      <td id=\"T_67e99_row21_col2\" class=\"data row21 col2\" >0.00</td>\n",
       "      <td id=\"T_67e99_row21_col3\" class=\"data row21 col3\" >-0.00</td>\n",
       "      <td id=\"T_67e99_row21_col4\" class=\"data row21 col4\" >0.00</td>\n",
       "      <td id=\"T_67e99_row21_col5\" class=\"data row21 col5\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67e99_level0_row22\" class=\"row_heading level0 row22\" >person_home_ownership_OWN</th>\n",
       "      <td id=\"T_67e99_row22_col0\" class=\"data row22 col0\" >0.01</td>\n",
       "      <td id=\"T_67e99_row22_col1\" class=\"data row22 col1\" >0.00</td>\n",
       "      <td id=\"T_67e99_row22_col2\" class=\"data row22 col2\" >0.01</td>\n",
       "      <td id=\"T_67e99_row22_col3\" class=\"data row22 col3\" >0.01</td>\n",
       "      <td id=\"T_67e99_row22_col4\" class=\"data row22 col4\" >0.01</td>\n",
       "      <td id=\"T_67e99_row22_col5\" class=\"data row22 col5\" >0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67e99_level0_row23\" class=\"row_heading level0 row23\" >person_home_ownership_RENT</th>\n",
       "      <td id=\"T_67e99_row23_col0\" class=\"data row23 col0\" >0.02</td>\n",
       "      <td id=\"T_67e99_row23_col1\" class=\"data row23 col1\" >0.00</td>\n",
       "      <td id=\"T_67e99_row23_col2\" class=\"data row23 col2\" >0.05</td>\n",
       "      <td id=\"T_67e99_row23_col3\" class=\"data row23 col3\" >0.05</td>\n",
       "      <td id=\"T_67e99_row23_col4\" class=\"data row23 col4\" >0.05</td>\n",
       "      <td id=\"T_67e99_row23_col5\" class=\"data row23 col5\" >0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67e99_level0_row24\" class=\"row_heading level0 row24\" >person_income</th>\n",
       "      <td id=\"T_67e99_row24_col0\" class=\"data row24 col0\" >0.00</td>\n",
       "      <td id=\"T_67e99_row24_col1\" class=\"data row24 col1\" >0.00</td>\n",
       "      <td id=\"T_67e99_row24_col2\" class=\"data row24 col2\" >0.06</td>\n",
       "      <td id=\"T_67e99_row24_col3\" class=\"data row24 col3\" >0.04</td>\n",
       "      <td id=\"T_67e99_row24_col4\" class=\"data row24 col4\" >0.05</td>\n",
       "      <td id=\"T_67e99_row24_col5\" class=\"data row24 col5\" >0.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x23398113810>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "cm = sns.light_palette(\"yellow\", as_cmap=True)\n",
    "\n",
    "def style_negative(v, props=''):\n",
    "    return props if v < 0 else None\n",
    "\n",
    "def highlight_max(s, props=''):\n",
    "    return np.where(s == np.nanmax(s.values), props, '')\n",
    "\n",
    "mean_cols = [col for col in feature_importance.columns if 'mean' in col]\n",
    "feat_mean_importance = round(feature_importance.set_index(\"feature\"), 3)[mean_cols]\n",
    "\n",
    "s2 = feat_mean_importance.style.text_gradient(cmap=cm)\\\n",
    "    .map(lambda v: 'opacity: 20%;' if (v < 0.01) and (v > -0.01) else None)\\\n",
    "        .format(precision=2)\n",
    "\n",
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Relevant_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.880799</td>\n",
       "      <td>0.955741</td>\n",
       "      <td>0.916741</td>\n",
       "      <td>0.864884</td>\n",
       "      <td>[loan_amnt, loan_int_rate, loan_percent_income...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.839908</td>\n",
       "      <td>0.967631</td>\n",
       "      <td>0.899257</td>\n",
       "      <td>0.831260</td>\n",
       "      <td>[person_emp_length, loan_int_rate, loan_grade_D]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.929997</td>\n",
       "      <td>0.923240</td>\n",
       "      <td>0.926606</td>\n",
       "      <td>0.886170</td>\n",
       "      <td>[person_age, person_income, person_emp_length,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.925985</td>\n",
       "      <td>0.990091</td>\n",
       "      <td>0.956966</td>\n",
       "      <td>0.930694</td>\n",
       "      <td>[person_income, person_emp_length, loan_int_ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.931365</td>\n",
       "      <td>0.987845</td>\n",
       "      <td>0.958774</td>\n",
       "      <td>0.933882</td>\n",
       "      <td>[person_age, person_income, person_emp_length,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.926250</td>\n",
       "      <td>0.993923</td>\n",
       "      <td>0.958894</td>\n",
       "      <td>0.933676</td>\n",
       "      <td>[person_age, person_income, person_emp_length,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Precision    Recall  f1_score  Accuracy  \\\n",
       "0  Logistic Regression   0.880799  0.955741  0.916741  0.864884   \n",
       "1                  SVM   0.839908  0.967631  0.899257  0.831260   \n",
       "2        Decision Tree   0.929997  0.923240  0.926606  0.886170   \n",
       "3        Random Forest   0.925985  0.990091  0.956966  0.930694   \n",
       "4              XGBoost   0.931365  0.987845  0.958774  0.933882   \n",
       "5             LightGBM   0.926250  0.993923  0.958894  0.933676   \n",
       "\n",
       "                                   Relevant_features  \n",
       "0  [loan_amnt, loan_int_rate, loan_percent_income...  \n",
       "1   [person_emp_length, loan_int_rate, loan_grade_D]  \n",
       "2  [person_age, person_income, person_emp_length,...  \n",
       "3  [person_income, person_emp_length, loan_int_ra...  \n",
       "4  [person_age, person_income, person_emp_length,...  \n",
       "5  [person_age, person_income, person_emp_length,...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17758, number of negative: 4933\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000497 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 665\n",
      "[LightGBM] [Info] Number of data points in the train set: 22691, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.782601 -> initscore=1.280889\n",
      "[LightGBM] [Info] Start training from score 1.280889\n",
      "                 Model  Precision    Recall  f1_score  Accuracy\n",
      "0  Logistic Regression   0.873765  0.958383  0.914120  0.859846\n",
      "1                  SVM   0.822712  0.961950  0.886899  0.809049\n",
      "2        Decision Tree   0.930631  0.925221  0.927918  0.888123\n",
      "3        Random Forest   0.924360  0.988109  0.955172  0.927815\n",
      "4              XGBoost   0.930450  0.986260  0.957542  0.931928\n",
      "5             LightGBM   0.925188  0.993394  0.958078  0.932339\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# List of models to evaluate\n",
    "models = [\n",
    "    (\"Logistic Regression\", LogisticRegression()),\n",
    "    (\"SVM\", SVC()),\n",
    "    (\"Decision Tree\", DecisionTreeClassifier()),\n",
    "    (\"Random Forest\", RandomForestClassifier()),\n",
    "    (\"XGBoost\", XGBClassifier()),\n",
    "    (\"LightGBM\", LGBMClassifier())\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "feature_importance = pd.DataFrame()\n",
    "# Iterate through models\n",
    "for model_name, model in models:\n",
    "    # Train the model\n",
    "    model.fit(X_train[results_df[results_df[\"Model\"] == model_name][\"Relevant_features\"].values[0]], y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test[results_df[results_df[\"Model\"] == model_name][\"Relevant_features\"].values[0]])\n",
    "    \n",
    "    # Evaluate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Append results to the list\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"Accuracy\": accuracy\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_reduced_features = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "print(results_reduced_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Relevant_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.880799</td>\n",
       "      <td>0.955741</td>\n",
       "      <td>0.916741</td>\n",
       "      <td>0.864884</td>\n",
       "      <td>[loan_amnt, loan_int_rate, loan_percent_income...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.839908</td>\n",
       "      <td>0.967631</td>\n",
       "      <td>0.899257</td>\n",
       "      <td>0.831260</td>\n",
       "      <td>[person_emp_length, loan_int_rate, loan_grade_D]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.929997</td>\n",
       "      <td>0.923240</td>\n",
       "      <td>0.926606</td>\n",
       "      <td>0.886170</td>\n",
       "      <td>[person_age, person_income, person_emp_length,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.925985</td>\n",
       "      <td>0.990091</td>\n",
       "      <td>0.956966</td>\n",
       "      <td>0.930694</td>\n",
       "      <td>[person_income, person_emp_length, loan_int_ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.931365</td>\n",
       "      <td>0.987845</td>\n",
       "      <td>0.958774</td>\n",
       "      <td>0.933882</td>\n",
       "      <td>[person_age, person_income, person_emp_length,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.926250</td>\n",
       "      <td>0.993923</td>\n",
       "      <td>0.958894</td>\n",
       "      <td>0.933676</td>\n",
       "      <td>[person_age, person_income, person_emp_length,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Precision    Recall  f1_score  Accuracy  \\\n",
       "0  Logistic Regression   0.880799  0.955741  0.916741  0.864884   \n",
       "1                  SVM   0.839908  0.967631  0.899257  0.831260   \n",
       "2        Decision Tree   0.929997  0.923240  0.926606  0.886170   \n",
       "3        Random Forest   0.925985  0.990091  0.956966  0.930694   \n",
       "4              XGBoost   0.931365  0.987845  0.958774  0.933882   \n",
       "5             LightGBM   0.926250  0.993923  0.958894  0.933676   \n",
       "\n",
       "                                   Relevant_features  \n",
       "0  [loan_amnt, loan_int_rate, loan_percent_income...  \n",
       "1   [person_emp_length, loan_int_rate, loan_grade_D]  \n",
       "2  [person_age, person_income, person_emp_length,...  \n",
       "3  [person_income, person_emp_length, loan_int_ra...  \n",
       "4  [person_age, person_income, person_emp_length,...  \n",
       "5  [person_age, person_income, person_emp_length,...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.873765</td>\n",
       "      <td>0.958383</td>\n",
       "      <td>0.914120</td>\n",
       "      <td>0.859846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.822712</td>\n",
       "      <td>0.961950</td>\n",
       "      <td>0.886899</td>\n",
       "      <td>0.809049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.930631</td>\n",
       "      <td>0.925221</td>\n",
       "      <td>0.927918</td>\n",
       "      <td>0.888123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.924360</td>\n",
       "      <td>0.988109</td>\n",
       "      <td>0.955172</td>\n",
       "      <td>0.927815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.930450</td>\n",
       "      <td>0.986260</td>\n",
       "      <td>0.957542</td>\n",
       "      <td>0.931928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.925188</td>\n",
       "      <td>0.993394</td>\n",
       "      <td>0.958078</td>\n",
       "      <td>0.932339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Precision    Recall  f1_score  Accuracy\n",
       "0  Logistic Regression   0.873765  0.958383  0.914120  0.859846\n",
       "1                  SVM   0.822712  0.961950  0.886899  0.809049\n",
       "2        Decision Tree   0.930631  0.925221  0.927918  0.888123\n",
       "3        Random Forest   0.924360  0.988109  0.955172  0.927815\n",
       "4              XGBoost   0.930450  0.986260  0.957542  0.931928\n",
       "5             LightGBM   0.925188  0.993394  0.958078  0.932339"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_reduced_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annak\\miniconda3\\envs\\loan_approval\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:12:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17758, number of negative: 4933\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 937\n",
      "[LightGBM] [Info] Number of data points in the train set: 22691, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.782601 -> initscore=1.280889\n",
      "[LightGBM] [Info] Start training from score 1.280889\n",
      "Voting Ensemble Accuracy: 0.9351\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define base models\n",
    "models = [\n",
    "    (\"Random Forest\", RandomForestClassifier(random_state=42)),\n",
    "    (\"XGBoost\", XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)),\n",
    "    (\"LightGBM\", LGBMClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "# Create Voting Classifier\n",
    "voting_clf = VotingClassifier(estimators=models, voting='soft')  # 'soft' uses predicted probabilities\n",
    "\n",
    "# Train on the training set\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Voting Ensemble Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RandomizedSearchCV for Random Forest...\n",
      "Running RandomizedSearchCV for XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annak\\miniconda3\\envs\\loan_approval\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "c:\\Users\\annak\\miniconda3\\envs\\loan_approval\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:13:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RandomizedSearchCV for LightGBM...\n",
      "[LightGBM] [Info] Number of positive: 17758, number of negative: 4933\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000524 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 937\n",
      "[LightGBM] [Info] Number of data points in the train set: 22691, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.782601 -> initscore=1.280889\n",
      "[LightGBM] [Info] Start training from score 1.280889\n",
      "\n",
      "Hyperparameter Search Results:\n",
      "           Model                                        Best Params  \\\n",
      "0  Random Forest  {'n_estimators': 200, 'min_samples_split': 2, ...   \n",
      "1        XGBoost  {'subsample': 1.0, 'n_estimators': 200, 'max_d...   \n",
      "2       LightGBM  {'subsample': 0.8, 'num_leaves': 50, 'n_estima...   \n",
      "\n",
      "   Test Accuracy  \n",
      "0       0.930900  \n",
      "1       0.934499  \n",
      "2       0.933779  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "param_grid_lgbm = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [-1, 10, 20],\n",
    "    'num_leaves': [31, 50, 100],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Create models and parameter grids\n",
    "models = {\n",
    "    'Random Forest': (RandomForestClassifier(random_state=42), param_grid_rf),\n",
    "    'XGBoost': (XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42), param_grid_xgb),\n",
    "    'LightGBM': (LGBMClassifier(random_state=42), param_grid_lgbm)\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV for each model\n",
    "best_estimators = {}\n",
    "results = []\n",
    "\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "    print(f\"Running RandomizedSearchCV for {model_name}...\")\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=20,  # Number of random combinations\n",
    "        cv=3,       # 3-fold cross-validation\n",
    "        scoring='accuracy',\n",
    "        random_state=42,\n",
    "        n_jobs=-1   # Use all CPU cores\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    best_estimators[model_name] = search.best_estimator_\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    y_pred = search.best_estimator_.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Best Params': search.best_params_,\n",
    "        'Test Accuracy': accuracy\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nHyperparameter Search Results:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annak\\miniconda3\\envs\\loan_approval\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:13:34] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17758, number of negative: 4933\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000571 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 937\n",
      "[LightGBM] [Info] Number of data points in the train set: 22691, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.782601 -> initscore=1.280889\n",
      "[LightGBM] [Info] Start training from score 1.280889\n",
      "Voting Ensemble Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define base models\n",
    "models = [\n",
    "    (\"Random Forest\", RandomForestClassifier(max_depth=30, n_estimators=200, random_state=42)),\n",
    "    (\"XGBoost\", XGBClassifier(use_label_encoder=False, eval_metric='logloss', subsample= 1.0, n_estimators= 200, max_depth= 6, learning_rate=0.1, colsample_bytree= 1.0, random_state=42)),\n",
    "    (\"LightGBM\", LGBMClassifier(max_depth=20, n_estimators=200, num_leaves=50, random_state=42, subsample=0.8))\n",
    "]\n",
    "\n",
    "# Create Voting Classifier\n",
    "voting_clf = VotingClassifier(estimators=models, voting='soft')  # 'soft' uses predicted probabilities\n",
    "\n",
    "# Train on the training set\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Voting Ensemble Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.935012853470437"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. duplicates:  165\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(data_folder + 'credit_risk_dataset.csv')  \n",
    "\n",
    "# Identify duplicate rows\n",
    "duplicates = df.duplicated(keep='first')\n",
    "num_duplicates = duplicates.sum()\n",
    "\n",
    "# Remove duplicate rows\n",
    "df = df.drop_duplicates(keep='first')\n",
    "\n",
    "print('no. duplicates: ', num_duplicates)\n",
    "\n",
    "df['person_emp_length'] = df['person_emp_length'].fillna(-1)\n",
    "df['loan_int_rate'] = df['loan_int_rate'].fillna(-1)\n",
    "\n",
    "# We're also adding 'missing indicator' fields to explicity call out rows with missing data:\n",
    "df['missing_emp_length'] = (df['person_emp_length'] == -1).astype(int)\n",
    "df['missing_loan_rate'] = (df['loan_int_rate'] == -1).astype(int)\n",
    "\n",
    "df['person_age'] = df['person_age'].astype('uint8')\n",
    "df['person_income'] = df['person_income'].astype('uint32')\n",
    "df['loan_amnt'] = df['loan_amnt'].astype('uint32')\n",
    "df['loan_int_rate'] = df['loan_int_rate'].astype('float32')\n",
    "df['loan_status'] = df['loan_status'].astype('uint8')\n",
    "df['loan_percent_income'] = df['loan_percent_income'].astype('float32')\n",
    "df['cb_person_cred_hist_length'] = df['cb_person_cred_hist_length'].astype('uint8')\n",
    "df['missing_emp_length'] = df['missing_emp_length'].astype('uint8')\n",
    "df['missing_loan_rate'] = df['missing_loan_rate'].astype('uint8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_age</th>\n",
       "      <th>person_income</th>\n",
       "      <th>person_home_ownership</th>\n",
       "      <th>person_emp_length</th>\n",
       "      <th>loan_intent</th>\n",
       "      <th>loan_grade</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>loan_int_rate</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>loan_percent_income</th>\n",
       "      <th>cb_person_default_on_file</th>\n",
       "      <th>cb_person_cred_hist_length</th>\n",
       "      <th>missing_emp_length</th>\n",
       "      <th>missing_loan_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>59000</td>\n",
       "      <td>RENT</td>\n",
       "      <td>123.0</td>\n",
       "      <td>PERSONAL</td>\n",
       "      <td>D</td>\n",
       "      <td>35000</td>\n",
       "      <td>16.02</td>\n",
       "      <td>1</td>\n",
       "      <td>0.59</td>\n",
       "      <td>Y</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>9600</td>\n",
       "      <td>OWN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>B</td>\n",
       "      <td>1000</td>\n",
       "      <td>11.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>N</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>9600</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MEDICAL</td>\n",
       "      <td>C</td>\n",
       "      <td>5500</td>\n",
       "      <td>12.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.57</td>\n",
       "      <td>N</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>65500</td>\n",
       "      <td>RENT</td>\n",
       "      <td>4.0</td>\n",
       "      <td>MEDICAL</td>\n",
       "      <td>C</td>\n",
       "      <td>35000</td>\n",
       "      <td>15.23</td>\n",
       "      <td>1</td>\n",
       "      <td>0.53</td>\n",
       "      <td>N</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>54400</td>\n",
       "      <td>RENT</td>\n",
       "      <td>8.0</td>\n",
       "      <td>MEDICAL</td>\n",
       "      <td>C</td>\n",
       "      <td>35000</td>\n",
       "      <td>14.27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.55</td>\n",
       "      <td>Y</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   person_age  person_income person_home_ownership  person_emp_length  \\\n",
       "0          22          59000                  RENT              123.0   \n",
       "1          21           9600                   OWN                5.0   \n",
       "2          25           9600              MORTGAGE                1.0   \n",
       "3          23          65500                  RENT                4.0   \n",
       "4          24          54400                  RENT                8.0   \n",
       "\n",
       "  loan_intent loan_grade  loan_amnt  loan_int_rate  loan_status  \\\n",
       "0    PERSONAL          D      35000          16.02            1   \n",
       "1   EDUCATION          B       1000          11.14            0   \n",
       "2     MEDICAL          C       5500          12.87            1   \n",
       "3     MEDICAL          C      35000          15.23            1   \n",
       "4     MEDICAL          C      35000          14.27            1   \n",
       "\n",
       "   loan_percent_income cb_person_default_on_file  cb_person_cred_hist_length  \\\n",
       "0                 0.59                         Y                           3   \n",
       "1                 0.10                         N                           2   \n",
       "2                 0.57                         N                           3   \n",
       "3                 0.53                         N                           2   \n",
       "4                 0.55                         Y                           4   \n",
       "\n",
       "   missing_emp_length  missing_loan_rate  \n",
       "0                   0                  0  \n",
       "1                   0                  0  \n",
       "2                   0                  0  \n",
       "3                   0                  0  \n",
       "4                   0                  0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df = df.drop(df[df['person_age'] > 100].index)\n",
    "df = df.drop(df[df['person_emp_length'] > 100].index)\n",
    "\n",
    "df['loantoincome'] = (df['loan_amnt'] / df['person_income']) - df['loan_percent_income']\n",
    "df['income'] = np.log(df['person_income'])\n",
    "\n",
    "# # df[\"loan_percent_incometoincome\"] = round((df[\"loan_percent_income\"] / df[\"person_income\"]), 8)\n",
    "# df['person_age_to_person_income'] = round(df['person_age'] / df['person_income'], 8)\n",
    "# df['person_emp_length_to_person_age'] = round(df['person_emp_length'] / df['person_age'], 8)\n",
    "# df['loan_int_rate_to_loan_amnt'] = round(df['loan_int_rate'] / df['loan_amnt'], 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annak\\AppData\\Local\\Temp\\ipykernel_30996\\203423103.py:20: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_final['cb_person_default_on_file'] = df_final['cb_person_default_on_file'].replace({'Y': 1, 'N': 0})\n"
     ]
    }
   ],
   "source": [
    "# We are using min-max scaling for continuous variables as the underlying data is not normally distributed\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "df[['person_age', 'person_income','loan_amnt']] = min_max_scaler.fit_transform(df[['person_age','person_income','loan_amnt']])\n",
    "df.sample(5)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "\n",
    "encoded_data = encoder.fit_transform(df[['person_home_ownership','loan_intent','loan_grade']])\n",
    "\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['person_home_ownership','loan_intent','loan_grade']))\n",
    "df_final = pd.concat([df.drop(['person_home_ownership','loan_intent','loan_grade'], axis=1).reset_index(drop=True) , encoded_df.reset_index(drop=True) ], axis=1)\n",
    "\n",
    "df_final.sample(5)\n",
    "\n",
    "# Converting Ys and Ns to 1s and 0s\n",
    "df_final['cb_person_default_on_file'] = df_final['cb_person_default_on_file'].replace({'Y': 1, 'N': 0})\n",
    "df_final.head()\n",
    "\n",
    "# Mark NaN values in person_emp_length and loan_int_rate\n",
    "\n",
    "df_final['missing_emp_length'] = df_final['person_emp_length'].isnull().astype(int)\n",
    "df_final['missing_int_rate'] = df_final['loan_int_rate'].isnull().astype(int)\n",
    "\n",
    "# Also mark loans with 0 interest as a missing value\n",
    "df_final.loc[df_final['loan_int_rate'] == 0, 'missing_int_rate'] = 1\n",
    "\n",
    "# Let NaNs equal -1 to indicate missing value\n",
    "df_final.loc[df_final['person_emp_length'].isnull(), 'person_emp_length'] = -1\n",
    "df_final.loc[df_final['loan_int_rate'].isnull(), 'loan_int_rate'] = -1\n",
    "\n",
    "df_final[\"loan_rejected\"] =  1 - df_final['loan_status']\n",
    "df_final = df_final.drop(['loan_status'], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the data (features and labels)\n",
    "X = df_final.drop('loan_rejected', axis=1)  # Features\n",
    "y = df_final['loan_rejected']  # Target label\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annak\\miniconda3\\envs\\loan_approval\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [10:16:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17716, number of negative: 4970\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000561 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2189\n",
      "[LightGBM] [Info] Number of data points in the train set: 22686, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.780922 -> initscore=1.271048\n",
      "[LightGBM] [Info] Start training from score 1.271048\n",
      "Voting Ensemble Accuracy: 0.9399\n"
     ]
    }
   ],
   "source": [
    "# Define base models\n",
    "models = [\n",
    "    (\"Random Forest\", RandomForestClassifier(random_state=42)),\n",
    "    (\"XGBoost\", XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)),\n",
    "    (\"LightGBM\", LGBMClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "# Create Voting Classifier\n",
    "voting_clf = VotingClassifier(estimators=models, voting='soft')  # 'soft' uses predicted probabilities\n",
    "\n",
    "# Train on the training set\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Voting Ensemble Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annak\\miniconda3\\envs\\loan_approval\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17716, number of negative: 4970\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002193 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2189\n",
      "[LightGBM] [Info] Number of data points in the train set: 22686, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.780922 -> initscore=1.271048\n",
      "[LightGBM] [Info] Start training from score 1.271048\n",
      "                 Model  Precision    Recall  f1_score  Accuracy\n",
      "0  Logistic Regression   0.883633  0.950559  0.915875  0.863417\n",
      "1                  SVM   0.844737  0.970809  0.903396  0.837602\n",
      "2        Decision Tree   0.937517  0.929257  0.933369  0.896225\n",
      "3        Random Forest   0.933308  0.988166  0.959954  0.935514\n",
      "4              XGBoost   0.935837  0.989612  0.961974  0.938805\n",
      "5             LightGBM   0.932699  0.993162  0.961982  0.938599\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# List of models to evaluate\n",
    "models = [\n",
    "    (\"Logistic Regression\", LogisticRegression()),\n",
    "    (\"SVM\", SVC()),\n",
    "    (\"Decision Tree\", DecisionTreeClassifier()),\n",
    "    (\"Random Forest\", RandomForestClassifier()),\n",
    "    (\"XGBoost\", XGBClassifier()),\n",
    "    (\"LightGBM\", LGBMClassifier())\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "feature_importance = pd.DataFrame()\n",
    "# Iterate through models\n",
    "for model_name, model in models:\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Append results to the list\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"Accuracy\": accuracy\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annak\\miniconda3\\envs\\loan_approval\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Iterate through models\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\annak\\miniconda3\\envs\\loan_approval\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\annak\\miniconda3\\envs\\loan_approval\\Lib\\site-packages\\sklearn\\svm\\_base.py:250\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    249\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[1;32m--> 250\u001b[0m fit(X, y, sample_weight, solver_type, kernel, random_seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[1;32mc:\\Users\\annak\\miniconda3\\envs\\loan_approval\\Lib\\site-packages\\sklearn\\svm\\_base.py:328\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    314\u001b[0m libsvm\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;66;03m# add other parameters to __init__\u001b[39;00m\n\u001b[0;32m    318\u001b[0m (\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_coef_,\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_status_,\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_iter,\n\u001b[1;32m--> 328\u001b[0m ) \u001b[38;5;241m=\u001b[39m libsvm\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    329\u001b[0m     X,\n\u001b[0;32m    330\u001b[0m     y,\n\u001b[0;32m    331\u001b[0m     svm_type\u001b[38;5;241m=\u001b[39msolver_type,\n\u001b[0;32m    332\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    333\u001b[0m     class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_weight_\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m0\u001b[39m)),\n\u001b[0;32m    334\u001b[0m     kernel\u001b[38;5;241m=\u001b[39mkernel,\n\u001b[0;32m    335\u001b[0m     C\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC,\n\u001b[0;32m    336\u001b[0m     nu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnu,\n\u001b[0;32m    337\u001b[0m     probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobability,\n\u001b[0;32m    338\u001b[0m     degree\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegree,\n\u001b[0;32m    339\u001b[0m     shrinking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshrinking,\n\u001b[0;32m    340\u001b[0m     tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol,\n\u001b[0;32m    341\u001b[0m     cache_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_size,\n\u001b[0;32m    342\u001b[0m     coef0\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef0,\n\u001b[0;32m    343\u001b[0m     gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gamma,\n\u001b[0;32m    344\u001b[0m     epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon,\n\u001b[0;32m    345\u001b[0m     max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter,\n\u001b[0;32m    346\u001b[0m     random_seed\u001b[38;5;241m=\u001b[39mrandom_seed,\n\u001b[0;32m    347\u001b[0m )\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_from_fit_status()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# List of models to evaluate\n",
    "models = [\n",
    "    (\"Logistic Regression\", LogisticRegression()),\n",
    "    (\"SVM\", SVC()),\n",
    "    (\"Decision Tree\", DecisionTreeClassifier()),\n",
    "    (\"Random Forest\", RandomForestClassifier()),\n",
    "    (\"XGBoost\", XGBClassifier()),\n",
    "    (\"LightGBM\", LGBMClassifier())\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "feature_importance = pd.DataFrame()\n",
    "# Iterate through models\n",
    "for model_name, model in models:\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42, scoring='accuracy')\n",
    "\n",
    "    # Accessing the results\n",
    "    importances_mean = result.importances_mean\n",
    "    importances_std = result.importances_std\n",
    "    importances = result.importances\n",
    "\n",
    "    # Create a DataFrame for better readability\n",
    "    import pandas as pd\n",
    "    model_feature_importances = pd.DataFrame({\n",
    "        'feature': X_test.columns,\n",
    "        model_name + '_mean': importances_mean,\n",
    "        model_name + '_std': importances_std\n",
    "        \n",
    "    })\n",
    "\n",
    "    relevent_features = model_feature_importances[(model_feature_importances[model_name + '_mean'] > 0.005)][\"feature\"].tolist()\n",
    "\n",
    "    # Append results to the list\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Relevant_features\": relevent_features\n",
    "    })\n",
    "\n",
    "    if feature_importance.shape[0] > 0:\n",
    "        feature_importance = feature_importance.merge(model_feature_importances, \"outer\", on=\"feature\")\n",
    "    else:\n",
    "        feature_importance = model_feature_importances.copy()\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "cm = sns.light_palette(\"yellow\", as_cmap=True)\n",
    "\n",
    "def style_negative(v, props=''):\n",
    "    return props if v < 0 else None\n",
    "\n",
    "def highlight_max(s, props=''):\n",
    "    return np.where(s == np.nanmax(s.values), props, '')\n",
    "\n",
    "mean_cols = [col for col in feature_importance.columns if 'mean' in col]\n",
    "feat_mean_importance = round(feature_importance.set_index(\"feature\"), 3)[mean_cols]\n",
    "\n",
    "s2 = feat_mean_importance.style.text_gradient(cmap=cm)\\\n",
    "    .map(lambda v: 'opacity: 20%;' if (v < 0.01) and (v > -0.01) else None)\\\n",
    "        .format(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e8886_row0_col0, #T_e8886_row1_col0, #T_e8886_row1_col3, #T_e8886_row1_col5, #T_e8886_row2_col4, #T_e8886_row2_col5, #T_e8886_row3_col3, #T_e8886_row3_col4, #T_e8886_row3_col5, #T_e8886_row4_col2, #T_e8886_row4_col4, #T_e8886_row4_col5, #T_e8886_row5_col1, #T_e8886_row8_col2, #T_e8886_row8_col5, #T_e8886_row11_col0, #T_e8886_row11_col5, #T_e8886_row13_col0, #T_e8886_row15_col0, #T_e8886_row18_col0, #T_e8886_row19_col0, #T_e8886_row20_col0, #T_e8886_row20_col2, #T_e8886_row20_col4, #T_e8886_row20_col5, #T_e8886_row21_col0, #T_e8886_row21_col2, #T_e8886_row21_col4, #T_e8886_row21_col5, #T_e8886_row22_col2, #T_e8886_row22_col4, #T_e8886_row22_col5, #T_e8886_row23_col0, #T_e8886_row24_col0, #T_e8886_row24_col5, #T_e8886_row25_col0, #T_e8886_row26_col0, #T_e8886_row27_col0, #T_e8886_row27_col2, #T_e8886_row27_col4, #T_e8886_row27_col5, #T_e8886_row30_col0 {\n",
       "  color: #f2f2dd;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_e8886_row0_col1, #T_e8886_row1_col1, #T_e8886_row3_col1, #T_e8886_row8_col1, #T_e8886_row9_col1, #T_e8886_row11_col1, #T_e8886_row14_col2, #T_e8886_row18_col1, #T_e8886_row19_col1, #T_e8886_row20_col1, #T_e8886_row21_col1, #T_e8886_row22_col1, #T_e8886_row23_col1, #T_e8886_row24_col1, #T_e8886_row26_col1, #T_e8886_row27_col1, #T_e8886_row28_col2, #T_e8886_row30_col1 {\n",
       "  color: #f3f3d1;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_e8886_row0_col2, #T_e8886_row4_col1, #T_e8886_row7_col1 {\n",
       "  color: #f2f2d7;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_e8886_row0_col3, #T_e8886_row4_col3, #T_e8886_row8_col3, #T_e8886_row20_col3, #T_e8886_row21_col3, #T_e8886_row22_col3, #T_e8886_row24_col3, #T_e8886_row27_col3 {\n",
       "  color: #f2f2db;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_e8886_row0_col4, #T_e8886_row0_col5, #T_e8886_row1_col4, #T_e8886_row8_col4, #T_e8886_row9_col2, #T_e8886_row11_col4, #T_e8886_row18_col5, #T_e8886_row26_col5 {\n",
       "  color: #f2f2dc;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_e8886_row1_col2 {\n",
       "  color: #f7f790;\n",
       "}\n",
       "#T_e8886_row2_col0 {\n",
       "  color: #f5f5a3;\n",
       "}\n",
       "#T_e8886_row2_col1 {\n",
       "  color: #f7f78b;\n",
       "}\n",
       "#T_e8886_row2_col2, #T_e8886_row10_col2 {\n",
       "  color: #fafa4f;\n",
       "}\n",
       "#T_e8886_row2_col3, #T_e8886_row5_col3, #T_e8886_row14_col3, #T_e8886_row15_col2, #T_e8886_row28_col3, #T_e8886_row30_col3 {\n",
       "  color: #f3f3cf;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_e8886_row3_col0, #T_e8886_row4_col0, #T_e8886_row8_col0, #T_e8886_row9_col0, #T_e8886_row14_col0 {\n",
       "  color: #f2f2da;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_e8886_row3_col2, #T_e8886_row18_col2 {\n",
       "  color: #f5f5a5;\n",
       "}\n",
       "#T_e8886_row5_col0 {\n",
       "  color: #f3f3ce;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_e8886_row5_col2, #T_e8886_row30_col4 {\n",
       "  color: #f8f880;\n",
       "}\n",
       "#T_e8886_row5_col4, #T_e8886_row26_col3 {\n",
       "  color: #f3f3d2;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_e8886_row5_col5, #T_e8886_row15_col5, #T_e8886_row25_col4 {\n",
       "  color: #f3f3c8;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_e8886_row6_col0, #T_e8886_row6_col4, #T_e8886_row22_col0 {\n",
       "  color: #f7f78c;\n",
       "}\n",
       "#T_e8886_row6_col1 {\n",
       "  color: #f5f5ab;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_e8886_row6_col2 {\n",
       "  color: #f7f78e;\n",
       "}\n",
       "#T_e8886_row6_col3 {\n",
       "  color: #f8f87a;\n",
       "}\n",
       "#T_e8886_row6_col5 {\n",
       "  color: #f6f699;\n",
       "}\n",
       "#T_e8886_row7_col0, #T_e8886_row12_col3, #T_e8886_row16_col3, #T_e8886_row25_col3 {\n",
       "  color: #f3f3c7;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_e8886_row7_col2, #T_e8886_row12_col2, #T_e8886_row13_col1, #T_e8886_row14_col1 {\n",
       "  color: #f3f3cb;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_e8886_row7_col3 {\n",
       "  color: #f2f2d5;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_e8886_row7_col4, #T_e8886_row15_col3, #T_e8886_row28_col5 {\n",
       "  color: #f3f3cd;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_e8886_row7_col5, #T_e8886_row9_col4, #T_e8886_row9_col5, #T_e8886_row14_col4, #T_e8886_row14_col5, #T_e8886_row18_col4, #T_e8886_row24_col4, #T_e8886_row26_col4 {\n",
       "  color: #f2f2d9;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_e8886_row9_col3, #T_e8886_row11_col3, #T_e8886_row18_col3, #T_e8886_row23_col3 {\n",
       "  color: #f2f2d8;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_e8886_row10_col0 {\n",
       "  color: #fcfc3a;\n",
       "}\n",
       "#T_e8886_row10_col1, #T_e8886_row17_col0, #T_e8886_row17_col2, #T_e8886_row17_col3, #T_e8886_row17_col4, #T_e8886_row17_col5 {\n",
       "  color: #ffff00;\n",
       "}\n",
       "#T_e8886_row10_col3, #T_e8886_row24_col2 {\n",
       "  color: #f7f788;\n",
       "}\n",
       "#T_e8886_row10_col4 {\n",
       "  color: #f6f69a;\n",
       "}\n",
       "#T_e8886_row10_col5, #T_e8886_row29_col2 {\n",
       "  color: #f8f879;\n",
       "}\n",
       "#T_e8886_row11_col2 {\n",
       "  color: #f4f4bf;\n",
       "}\n",
       "#T_e8886_row12_col0, #T_e8886_row28_col0 {\n",
       "  color: #f4f4c2;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_e8886_row12_col1 {\n",
       "  color: #f4f4b8;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_e8886_row12_col4, #T_e8886_row16_col4, #T_e8886_row23_col4, #T_e8886_row25_col5 {\n",
       "  color: #f4f4be;\n",
       "}\n",
       "#T_e8886_row12_col5, #T_e8886_row23_col5 {\n",
       "  color: #f4f4c2;\n",
       "}\n",
       "#T_e8886_row13_col2, #T_e8886_row29_col0 {\n",
       "  color: #f5f5a7;\n",
       "}\n",
       "#T_e8886_row13_col3 {\n",
       "  color: #f4f4b6;\n",
       "}\n",
       "#T_e8886_row13_col4 {\n",
       "  color: #f5f5a6;\n",
       "}\n",
       "#T_e8886_row13_col5 {\n",
       "  color: #f5f5a2;\n",
       "}\n",
       "#T_e8886_row15_col1, #T_e8886_row16_col1 {\n",
       "  color: #f3f3c4;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_e8886_row15_col4, #T_e8886_row28_col4 {\n",
       "  color: #f4f4c3;\n",
       "}\n",
       "#T_e8886_row16_col0 {\n",
       "  color: #f3f3ca;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_e8886_row16_col2 {\n",
       "  color: #f3f3c9;\n",
       "}\n",
       "#T_e8886_row16_col5 {\n",
       "  color: #f4f4c0;\n",
       "}\n",
       "#T_e8886_row17_col1, #T_e8886_row28_col1 {\n",
       "  color: #f4f4be;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_e8886_row19_col2 {\n",
       "  color: #f5f5b1;\n",
       "}\n",
       "#T_e8886_row19_col3 {\n",
       "  color: #f5f5b0;\n",
       "}\n",
       "#T_e8886_row19_col4 {\n",
       "  color: #f4f4b7;\n",
       "}\n",
       "#T_e8886_row19_col5, #T_e8886_row23_col2 {\n",
       "  color: #f4f4bb;\n",
       "}\n",
       "#T_e8886_row25_col1 {\n",
       "  color: #f5f5a5;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_e8886_row25_col2 {\n",
       "  color: #f4f4bd;\n",
       "}\n",
       "#T_e8886_row26_col2 {\n",
       "  color: #f3f3c5;\n",
       "}\n",
       "#T_e8886_row29_col1 {\n",
       "  color: #f5f5b1;\n",
       "  opacity: 20%;\n",
       "}\n",
       "#T_e8886_row29_col3 {\n",
       "  color: #fafa52;\n",
       "}\n",
       "#T_e8886_row29_col4 {\n",
       "  color: #f9f96e;\n",
       "}\n",
       "#T_e8886_row29_col5 {\n",
       "  color: #f9f962;\n",
       "}\n",
       "#T_e8886_row30_col2 {\n",
       "  color: #f8f873;\n",
       "}\n",
       "#T_e8886_row30_col5 {\n",
       "  color: #f7f78a;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e8886\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e8886_level0_col0\" class=\"col_heading level0 col0\" >Logistic Regression_mean</th>\n",
       "      <th id=\"T_e8886_level0_col1\" class=\"col_heading level0 col1\" >SVM_mean</th>\n",
       "      <th id=\"T_e8886_level0_col2\" class=\"col_heading level0 col2\" >Decision Tree_mean</th>\n",
       "      <th id=\"T_e8886_level0_col3\" class=\"col_heading level0 col3\" >Random Forest_mean</th>\n",
       "      <th id=\"T_e8886_level0_col4\" class=\"col_heading level0 col4\" >XGBoost_mean</th>\n",
       "      <th id=\"T_e8886_level0_col5\" class=\"col_heading level0 col5\" >LightGBM_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >feature</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row0\" class=\"row_heading level0 row0\" >cb_person_cred_hist_length</th>\n",
       "      <td id=\"T_e8886_row0_col0\" class=\"data row0 col0\" >-0.00</td>\n",
       "      <td id=\"T_e8886_row0_col1\" class=\"data row0 col1\" >-0.00</td>\n",
       "      <td id=\"T_e8886_row0_col2\" class=\"data row0 col2\" >0.00</td>\n",
       "      <td id=\"T_e8886_row0_col3\" class=\"data row0 col3\" >-0.00</td>\n",
       "      <td id=\"T_e8886_row0_col4\" class=\"data row0 col4\" >0.00</td>\n",
       "      <td id=\"T_e8886_row0_col5\" class=\"data row0 col5\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row1\" class=\"row_heading level0 row1\" >cb_person_default_on_file</th>\n",
       "      <td id=\"T_e8886_row1_col0\" class=\"data row1 col0\" >-0.00</td>\n",
       "      <td id=\"T_e8886_row1_col1\" class=\"data row1 col1\" >-0.00</td>\n",
       "      <td id=\"T_e8886_row1_col2\" class=\"data row1 col2\" >0.04</td>\n",
       "      <td id=\"T_e8886_row1_col3\" class=\"data row1 col3\" >-0.00</td>\n",
       "      <td id=\"T_e8886_row1_col4\" class=\"data row1 col4\" >0.00</td>\n",
       "      <td id=\"T_e8886_row1_col5\" class=\"data row1 col5\" >-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row2\" class=\"row_heading level0 row2\" >income</th>\n",
       "      <td id=\"T_e8886_row2_col0\" class=\"data row2 col0\" >0.01</td>\n",
       "      <td id=\"T_e8886_row2_col1\" class=\"data row2 col1\" >0.01</td>\n",
       "      <td id=\"T_e8886_row2_col2\" class=\"data row2 col2\" >0.07</td>\n",
       "      <td id=\"T_e8886_row2_col3\" class=\"data row2 col3\" >0.00</td>\n",
       "      <td id=\"T_e8886_row2_col4\" class=\"data row2 col4\" >0.00</td>\n",
       "      <td id=\"T_e8886_row2_col5\" class=\"data row2 col5\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row3\" class=\"row_heading level0 row3\" >loan_amnt</th>\n",
       "      <td id=\"T_e8886_row3_col0\" class=\"data row3 col0\" >0.00</td>\n",
       "      <td id=\"T_e8886_row3_col1\" class=\"data row3 col1\" >0.00</td>\n",
       "      <td id=\"T_e8886_row3_col2\" class=\"data row3 col2\" >0.03</td>\n",
       "      <td id=\"T_e8886_row3_col3\" class=\"data row3 col3\" >-0.00</td>\n",
       "      <td id=\"T_e8886_row3_col4\" class=\"data row3 col4\" >0.00</td>\n",
       "      <td id=\"T_e8886_row3_col5\" class=\"data row3 col5\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row4\" class=\"row_heading level0 row4\" >loan_grade_B</th>\n",
       "      <td id=\"T_e8886_row4_col0\" class=\"data row4 col0\" >0.00</td>\n",
       "      <td id=\"T_e8886_row4_col1\" class=\"data row4 col1\" >-0.00</td>\n",
       "      <td id=\"T_e8886_row4_col2\" class=\"data row4 col2\" >0.00</td>\n",
       "      <td id=\"T_e8886_row4_col3\" class=\"data row4 col3\" >0.00</td>\n",
       "      <td id=\"T_e8886_row4_col4\" class=\"data row4 col4\" >-0.00</td>\n",
       "      <td id=\"T_e8886_row4_col5\" class=\"data row4 col5\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row5\" class=\"row_heading level0 row5\" >loan_grade_C</th>\n",
       "      <td id=\"T_e8886_row5_col0\" class=\"data row5 col0\" >0.00</td>\n",
       "      <td id=\"T_e8886_row5_col1\" class=\"data row5 col1\" >-0.00</td>\n",
       "      <td id=\"T_e8886_row5_col2\" class=\"data row5 col2\" >0.05</td>\n",
       "      <td id=\"T_e8886_row5_col3\" class=\"data row5 col3\" >0.00</td>\n",
       "      <td id=\"T_e8886_row5_col4\" class=\"data row5 col4\" >0.01</td>\n",
       "      <td id=\"T_e8886_row5_col5\" class=\"data row5 col5\" >0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row6\" class=\"row_heading level0 row6\" >loan_grade_D</th>\n",
       "      <td id=\"T_e8886_row6_col0\" class=\"data row6 col0\" >0.02</td>\n",
       "      <td id=\"T_e8886_row6_col1\" class=\"data row6 col1\" >0.01</td>\n",
       "      <td id=\"T_e8886_row6_col2\" class=\"data row6 col2\" >0.04</td>\n",
       "      <td id=\"T_e8886_row6_col3\" class=\"data row6 col3\" >0.03</td>\n",
       "      <td id=\"T_e8886_row6_col4\" class=\"data row6 col4\" >0.03</td>\n",
       "      <td id=\"T_e8886_row6_col5\" class=\"data row6 col5\" >0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row7\" class=\"row_heading level0 row7\" >loan_grade_E</th>\n",
       "      <td id=\"T_e8886_row7_col0\" class=\"data row7 col0\" >0.01</td>\n",
       "      <td id=\"T_e8886_row7_col1\" class=\"data row7 col1\" >-0.00</td>\n",
       "      <td id=\"T_e8886_row7_col2\" class=\"data row7 col2\" >0.01</td>\n",
       "      <td id=\"T_e8886_row7_col3\" class=\"data row7 col3\" >0.00</td>\n",
       "      <td id=\"T_e8886_row7_col4\" class=\"data row7 col4\" >0.01</td>\n",
       "      <td id=\"T_e8886_row7_col5\" class=\"data row7 col5\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row8\" class=\"row_heading level0 row8\" >loan_grade_F</th>\n",
       "      <td id=\"T_e8886_row8_col0\" class=\"data row8 col0\" >0.00</td>\n",
       "      <td id=\"T_e8886_row8_col1\" class=\"data row8 col1\" >-0.00</td>\n",
       "      <td id=\"T_e8886_row8_col2\" class=\"data row8 col2\" >0.00</td>\n",
       "      <td id=\"T_e8886_row8_col3\" class=\"data row8 col3\" >-0.00</td>\n",
       "      <td id=\"T_e8886_row8_col4\" class=\"data row8 col4\" >0.00</td>\n",
       "      <td id=\"T_e8886_row8_col5\" class=\"data row8 col5\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row9\" class=\"row_heading level0 row9\" >loan_grade_G</th>\n",
       "      <td id=\"T_e8886_row9_col0\" class=\"data row9 col0\" >0.00</td>\n",
       "      <td id=\"T_e8886_row9_col1\" class=\"data row9 col1\" >0.00</td>\n",
       "      <td id=\"T_e8886_row9_col2\" class=\"data row9 col2\" >0.00</td>\n",
       "      <td id=\"T_e8886_row9_col3\" class=\"data row9 col3\" >0.00</td>\n",
       "      <td id=\"T_e8886_row9_col4\" class=\"data row9 col4\" >0.00</td>\n",
       "      <td id=\"T_e8886_row9_col5\" class=\"data row9 col5\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row10\" class=\"row_heading level0 row10\" >loan_int_rate</th>\n",
       "      <td id=\"T_e8886_row10_col0\" class=\"data row10 col0\" >0.04</td>\n",
       "      <td id=\"T_e8886_row10_col1\" class=\"data row10 col1\" >0.03</td>\n",
       "      <td id=\"T_e8886_row10_col2\" class=\"data row10 col2\" >0.07</td>\n",
       "      <td id=\"T_e8886_row10_col3\" class=\"data row10 col3\" >0.03</td>\n",
       "      <td id=\"T_e8886_row10_col4\" class=\"data row10 col4\" >0.03</td>\n",
       "      <td id=\"T_e8886_row10_col5\" class=\"data row10 col5\" >0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row11\" class=\"row_heading level0 row11\" >loan_int_rate_to_loan_amnt</th>\n",
       "      <td id=\"T_e8886_row11_col0\" class=\"data row11 col0\" >0.00</td>\n",
       "      <td id=\"T_e8886_row11_col1\" class=\"data row11 col1\" >0.00</td>\n",
       "      <td id=\"T_e8886_row11_col2\" class=\"data row11 col2\" >0.01</td>\n",
       "      <td id=\"T_e8886_row11_col3\" class=\"data row11 col3\" >0.00</td>\n",
       "      <td id=\"T_e8886_row11_col4\" class=\"data row11 col4\" >0.00</td>\n",
       "      <td id=\"T_e8886_row11_col5\" class=\"data row11 col5\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row12\" class=\"row_heading level0 row12\" >loan_intent_EDUCATION</th>\n",
       "      <td id=\"T_e8886_row12_col0\" class=\"data row12 col0\" >0.01</td>\n",
       "      <td id=\"T_e8886_row12_col1\" class=\"data row12 col1\" >0.00</td>\n",
       "      <td id=\"T_e8886_row12_col2\" class=\"data row12 col2\" >0.01</td>\n",
       "      <td id=\"T_e8886_row12_col3\" class=\"data row12 col3\" >0.01</td>\n",
       "      <td id=\"T_e8886_row12_col4\" class=\"data row12 col4\" >0.01</td>\n",
       "      <td id=\"T_e8886_row12_col5\" class=\"data row12 col5\" >0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row13\" class=\"row_heading level0 row13\" >loan_intent_HOMEIMPROVEMENT</th>\n",
       "      <td id=\"T_e8886_row13_col0\" class=\"data row13 col0\" >-0.00</td>\n",
       "      <td id=\"T_e8886_row13_col1\" class=\"data row13 col1\" >0.00</td>\n",
       "      <td id=\"T_e8886_row13_col2\" class=\"data row13 col2\" >0.03</td>\n",
       "      <td id=\"T_e8886_row13_col3\" class=\"data row13 col3\" >0.01</td>\n",
       "      <td id=\"T_e8886_row13_col4\" class=\"data row13 col4\" >0.02</td>\n",
       "      <td id=\"T_e8886_row13_col5\" class=\"data row13 col5\" >0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row14\" class=\"row_heading level0 row14\" >loan_intent_MEDICAL</th>\n",
       "      <td id=\"T_e8886_row14_col0\" class=\"data row14 col0\" >0.00</td>\n",
       "      <td id=\"T_e8886_row14_col1\" class=\"data row14 col1\" >0.00</td>\n",
       "      <td id=\"T_e8886_row14_col2\" class=\"data row14 col2\" >0.01</td>\n",
       "      <td id=\"T_e8886_row14_col3\" class=\"data row14 col3\" >0.00</td>\n",
       "      <td id=\"T_e8886_row14_col4\" class=\"data row14 col4\" >0.00</td>\n",
       "      <td id=\"T_e8886_row14_col5\" class=\"data row14 col5\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row15\" class=\"row_heading level0 row15\" >loan_intent_PERSONAL</th>\n",
       "      <td id=\"T_e8886_row15_col0\" class=\"data row15 col0\" >-0.00</td>\n",
       "      <td id=\"T_e8886_row15_col1\" class=\"data row15 col1\" >0.00</td>\n",
       "      <td id=\"T_e8886_row15_col2\" class=\"data row15 col2\" >0.01</td>\n",
       "      <td id=\"T_e8886_row15_col3\" class=\"data row15 col3\" >0.01</td>\n",
       "      <td id=\"T_e8886_row15_col4\" class=\"data row15 col4\" >0.01</td>\n",
       "      <td id=\"T_e8886_row15_col5\" class=\"data row15 col5\" >0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row16\" class=\"row_heading level0 row16\" >loan_intent_VENTURE</th>\n",
       "      <td id=\"T_e8886_row16_col0\" class=\"data row16 col0\" >0.01</td>\n",
       "      <td id=\"T_e8886_row16_col1\" class=\"data row16 col1\" >0.00</td>\n",
       "      <td id=\"T_e8886_row16_col2\" class=\"data row16 col2\" >0.01</td>\n",
       "      <td id=\"T_e8886_row16_col3\" class=\"data row16 col3\" >0.01</td>\n",
       "      <td id=\"T_e8886_row16_col4\" class=\"data row16 col4\" >0.01</td>\n",
       "      <td id=\"T_e8886_row16_col5\" class=\"data row16 col5\" >0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row17\" class=\"row_heading level0 row17\" >loan_percent_income</th>\n",
       "      <td id=\"T_e8886_row17_col0\" class=\"data row17 col0\" >0.06</td>\n",
       "      <td id=\"T_e8886_row17_col1\" class=\"data row17 col1\" >0.00</td>\n",
       "      <td id=\"T_e8886_row17_col2\" class=\"data row17 col2\" >0.11</td>\n",
       "      <td id=\"T_e8886_row17_col3\" class=\"data row17 col3\" >0.08</td>\n",
       "      <td id=\"T_e8886_row17_col4\" class=\"data row17 col4\" >0.09</td>\n",
       "      <td id=\"T_e8886_row17_col5\" class=\"data row17 col5\" >0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row18\" class=\"row_heading level0 row18\" >loan_percent_incometoincome</th>\n",
       "      <td id=\"T_e8886_row18_col0\" class=\"data row18 col0\" >0.00</td>\n",
       "      <td id=\"T_e8886_row18_col1\" class=\"data row18 col1\" >0.00</td>\n",
       "      <td id=\"T_e8886_row18_col2\" class=\"data row18 col2\" >0.03</td>\n",
       "      <td id=\"T_e8886_row18_col3\" class=\"data row18 col3\" >0.00</td>\n",
       "      <td id=\"T_e8886_row18_col4\" class=\"data row18 col4\" >0.00</td>\n",
       "      <td id=\"T_e8886_row18_col5\" class=\"data row18 col5\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row19\" class=\"row_heading level0 row19\" >loantoincome</th>\n",
       "      <td id=\"T_e8886_row19_col0\" class=\"data row19 col0\" >0.00</td>\n",
       "      <td id=\"T_e8886_row19_col1\" class=\"data row19 col1\" >0.00</td>\n",
       "      <td id=\"T_e8886_row19_col2\" class=\"data row19 col2\" >0.02</td>\n",
       "      <td id=\"T_e8886_row19_col3\" class=\"data row19 col3\" >0.01</td>\n",
       "      <td id=\"T_e8886_row19_col4\" class=\"data row19 col4\" >0.02</td>\n",
       "      <td id=\"T_e8886_row19_col5\" class=\"data row19 col5\" >0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row20\" class=\"row_heading level0 row20\" >missing_emp_length</th>\n",
       "      <td id=\"T_e8886_row20_col0\" class=\"data row20 col0\" >0.00</td>\n",
       "      <td id=\"T_e8886_row20_col1\" class=\"data row20 col1\" >0.00</td>\n",
       "      <td id=\"T_e8886_row20_col2\" class=\"data row20 col2\" >0.00</td>\n",
       "      <td id=\"T_e8886_row20_col3\" class=\"data row20 col3\" >0.00</td>\n",
       "      <td id=\"T_e8886_row20_col4\" class=\"data row20 col4\" >0.00</td>\n",
       "      <td id=\"T_e8886_row20_col5\" class=\"data row20 col5\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row21\" class=\"row_heading level0 row21\" >missing_int_rate</th>\n",
       "      <td id=\"T_e8886_row21_col0\" class=\"data row21 col0\" >0.00</td>\n",
       "      <td id=\"T_e8886_row21_col1\" class=\"data row21 col1\" >0.00</td>\n",
       "      <td id=\"T_e8886_row21_col2\" class=\"data row21 col2\" >0.00</td>\n",
       "      <td id=\"T_e8886_row21_col3\" class=\"data row21 col3\" >0.00</td>\n",
       "      <td id=\"T_e8886_row21_col4\" class=\"data row21 col4\" >0.00</td>\n",
       "      <td id=\"T_e8886_row21_col5\" class=\"data row21 col5\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row22\" class=\"row_heading level0 row22\" >missing_loan_rate</th>\n",
       "      <td id=\"T_e8886_row22_col0\" class=\"data row22 col0\" >0.02</td>\n",
       "      <td id=\"T_e8886_row22_col1\" class=\"data row22 col1\" >0.00</td>\n",
       "      <td id=\"T_e8886_row22_col2\" class=\"data row22 col2\" >0.00</td>\n",
       "      <td id=\"T_e8886_row22_col3\" class=\"data row22 col3\" >-0.00</td>\n",
       "      <td id=\"T_e8886_row22_col4\" class=\"data row22 col4\" >0.00</td>\n",
       "      <td id=\"T_e8886_row22_col5\" class=\"data row22 col5\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row23\" class=\"row_heading level0 row23\" >person_age</th>\n",
       "      <td id=\"T_e8886_row23_col0\" class=\"data row23 col0\" >-0.00</td>\n",
       "      <td id=\"T_e8886_row23_col1\" class=\"data row23 col1\" >-0.00</td>\n",
       "      <td id=\"T_e8886_row23_col2\" class=\"data row23 col2\" >0.02</td>\n",
       "      <td id=\"T_e8886_row23_col3\" class=\"data row23 col3\" >0.00</td>\n",
       "      <td id=\"T_e8886_row23_col4\" class=\"data row23 col4\" >0.01</td>\n",
       "      <td id=\"T_e8886_row23_col5\" class=\"data row23 col5\" >0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row24\" class=\"row_heading level0 row24\" >person_age_to_person_income</th>\n",
       "      <td id=\"T_e8886_row24_col0\" class=\"data row24 col0\" >0.00</td>\n",
       "      <td id=\"T_e8886_row24_col1\" class=\"data row24 col1\" >0.00</td>\n",
       "      <td id=\"T_e8886_row24_col2\" class=\"data row24 col2\" >0.04</td>\n",
       "      <td id=\"T_e8886_row24_col3\" class=\"data row24 col3\" >0.00</td>\n",
       "      <td id=\"T_e8886_row24_col4\" class=\"data row24 col4\" >0.00</td>\n",
       "      <td id=\"T_e8886_row24_col5\" class=\"data row24 col5\" >-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row25\" class=\"row_heading level0 row25\" >person_emp_length</th>\n",
       "      <td id=\"T_e8886_row25_col0\" class=\"data row25 col0\" >-0.00</td>\n",
       "      <td id=\"T_e8886_row25_col1\" class=\"data row25 col1\" >0.01</td>\n",
       "      <td id=\"T_e8886_row25_col2\" class=\"data row25 col2\" >0.02</td>\n",
       "      <td id=\"T_e8886_row25_col3\" class=\"data row25 col3\" >0.01</td>\n",
       "      <td id=\"T_e8886_row25_col4\" class=\"data row25 col4\" >0.01</td>\n",
       "      <td id=\"T_e8886_row25_col5\" class=\"data row25 col5\" >0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row26\" class=\"row_heading level0 row26\" >person_emp_length_to_person_age</th>\n",
       "      <td id=\"T_e8886_row26_col0\" class=\"data row26 col0\" >0.00</td>\n",
       "      <td id=\"T_e8886_row26_col1\" class=\"data row26 col1\" >-0.00</td>\n",
       "      <td id=\"T_e8886_row26_col2\" class=\"data row26 col2\" >0.01</td>\n",
       "      <td id=\"T_e8886_row26_col3\" class=\"data row26 col3\" >0.00</td>\n",
       "      <td id=\"T_e8886_row26_col4\" class=\"data row26 col4\" >0.00</td>\n",
       "      <td id=\"T_e8886_row26_col5\" class=\"data row26 col5\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row27\" class=\"row_heading level0 row27\" >person_home_ownership_OTHER</th>\n",
       "      <td id=\"T_e8886_row27_col0\" class=\"data row27 col0\" >-0.00</td>\n",
       "      <td id=\"T_e8886_row27_col1\" class=\"data row27 col1\" >-0.00</td>\n",
       "      <td id=\"T_e8886_row27_col2\" class=\"data row27 col2\" >0.00</td>\n",
       "      <td id=\"T_e8886_row27_col3\" class=\"data row27 col3\" >0.00</td>\n",
       "      <td id=\"T_e8886_row27_col4\" class=\"data row27 col4\" >0.00</td>\n",
       "      <td id=\"T_e8886_row27_col5\" class=\"data row27 col5\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row28\" class=\"row_heading level0 row28\" >person_home_ownership_OWN</th>\n",
       "      <td id=\"T_e8886_row28_col0\" class=\"data row28 col0\" >0.01</td>\n",
       "      <td id=\"T_e8886_row28_col1\" class=\"data row28 col1\" >0.00</td>\n",
       "      <td id=\"T_e8886_row28_col2\" class=\"data row28 col2\" >0.01</td>\n",
       "      <td id=\"T_e8886_row28_col3\" class=\"data row28 col3\" >0.00</td>\n",
       "      <td id=\"T_e8886_row28_col4\" class=\"data row28 col4\" >0.01</td>\n",
       "      <td id=\"T_e8886_row28_col5\" class=\"data row28 col5\" >0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row29\" class=\"row_heading level0 row29\" >person_home_ownership_RENT</th>\n",
       "      <td id=\"T_e8886_row29_col0\" class=\"data row29 col0\" >0.01</td>\n",
       "      <td id=\"T_e8886_row29_col1\" class=\"data row29 col1\" >0.01</td>\n",
       "      <td id=\"T_e8886_row29_col2\" class=\"data row29 col2\" >0.05</td>\n",
       "      <td id=\"T_e8886_row29_col3\" class=\"data row29 col3\" >0.05</td>\n",
       "      <td id=\"T_e8886_row29_col4\" class=\"data row29 col4\" >0.05</td>\n",
       "      <td id=\"T_e8886_row29_col5\" class=\"data row29 col5\" >0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8886_level0_row30\" class=\"row_heading level0 row30\" >person_income</th>\n",
       "      <td id=\"T_e8886_row30_col0\" class=\"data row30 col0\" >-0.00</td>\n",
       "      <td id=\"T_e8886_row30_col1\" class=\"data row30 col1\" >0.00</td>\n",
       "      <td id=\"T_e8886_row30_col2\" class=\"data row30 col2\" >0.05</td>\n",
       "      <td id=\"T_e8886_row30_col3\" class=\"data row30 col3\" >0.00</td>\n",
       "      <td id=\"T_e8886_row30_col4\" class=\"data row30 col4\" >0.04</td>\n",
       "      <td id=\"T_e8886_row30_col5\" class=\"data row30 col5\" >0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x233f8309cd0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4675026,
     "sourceId": 7949759,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
